{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be347187",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Ling 450/807 SFU - Assignment 1\n",
    "\n",
    "Virginia Uhi, Eunice Wong, & Han Zhang\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cd57cf",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Import packages\n",
    "\n",
    "We import everything we will need here at the beginning and load the spaCy language model. Note that we are using the small English model. One thing you could try is to download and load [other models for English](https://spacy.io/models/en) and compare the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65927718",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing Spacy \n",
    "import os\n",
    "import spacy\n",
    "import re\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d845f6",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Approach 1: Using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c2aea16-5b94-4392-a32b-d2ec31f72361",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m five_files \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA1_data/5c1548a31e67d78e2771624f.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA1_data/5c489df91e67d78e271d66c5.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA1_data/5c182ac21e67d78e277944ad.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA1_data/5c28972a795bd2fac69fa974.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA1_data/5c29beda1e67d78e27b74939.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Process five text files at the same time, and have all the quotes in the five files can be extracted and put in the same file\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m (five_files, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m output:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_quotes\u001b[39m(text):\n\u001b[1;32m     10\u001b[0m         quotes \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m“](.*?)[”\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m, text) \n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:279\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(io_open)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_modified_open\u001b[39m(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m         )\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# The starter file loads and processes only one file at a time. We are using the command to do 5 files at the same time\n",
    "\n",
    "# First we take five text files out \n",
    "\n",
    "five_files = [\"A1_data/5c1548a31e67d78e2771624f.txt\", \"A1_data/5c489df91e67d78e271d66c5.txt\", \"A1_data/5c182ac21e67d78e277944ad.txt\", \"A1_data/5c28972a795bd2fac69fa974.txt\", \"A1_data/5c29beda1e67d78e27b74939.txt\"]\n",
    "\n",
    "# Process five text files at the same time, and have all the quotes in the five files can be extracted and put in the same file\n",
    "with open (five_files, 'w', encoding = \"utf-8\") as output:\n",
    "    def get_quotes(text):\n",
    "        quotes = re.findall(r'[\"“](.*?)[”\"]', text) \n",
    "            # so that every time as soon as Spacy identify any quotation marks (either straight or curly), the quoted content with the quotation mark will be extracted\n",
    "        return(quotes)\n",
    "    \n",
    "    for file_path in five_files: \n",
    "        output.write(file_path + \"\\n\")\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "                # The starter code seperate the text into sentences at the begining; However, for the sake of better identifying the sentence, we decide to not seperate the sentences first\n",
    "\n",
    "            found_quotes = get_quotes(text) \n",
    "            print(found_quotes)\n",
    "            for quote in found_quotes:\n",
    "                output.write(str(quote) + \"\\n\")  # Using the \"\\n\" to make the quotes listed line by line\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25227db3",
   "metadata": {},
   "source": [
    "## Approach 2: Using spaCy's Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b207ae63",
   "metadata": {},
   "source": [
    "This approach is based on notebooks by [William J.B. Mattingly](https://wjbmattingly.com/). His book, Introduction to Python for Humanists, is available online from the [SFU Library](https://sfu-primo.hosted.exlibrisgroup.com/permalink/f/usv8m3/01SFUL_ALMA51476999620003611). \n",
    "\n",
    "For more on spaCy's Matcher, see Advanced NLP with spaCy, [chapter 2](https://course.spacy.io/en/chapter2)). \n",
    "\n",
    "We have already loaded everything we need at the beginning of this notebook (imported Matcher, assigned it to a `matcher` object), so now we can use it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f0879",
   "metadata": {},
   "source": [
    "## Finding quotes and speakers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ee2f7c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Finding proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd311bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is optional. It just tells you who are the people mentioned. You can use it later if you want to find out the speakers of the quotes\n",
    "\n",
    "# matcher = Matcher(nlp.vocab)\n",
    "# pattern_n = [{\"POS\": \"PROPN\"}]\n",
    "# matcher.add(\"PROPER_NOUNS\", [pattern_n], greedy=\"LONGEST\")\n",
    "# doc = nlp(text)\n",
    "# matches = matcher(doc)\n",
    "# print (len(matches))\n",
    "# for match in matches[:10]:\n",
    "    #print (match, doc[match[1]:match[2]])\n",
    "    \n",
    "## You can try to extract full names by adding multi-word nouns, http://spacy.pythonhumanities.com/02_02_matcher.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "debdbaaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m pattern_n \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOS\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROPN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOP\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m\"\u001b[39m }] \u001b[38;5;66;03m# with OP it will look for propoer noun one or more times\u001b[39;00m\n\u001b[1;32m      6\u001b[0m matcher\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROPER_NOUNS\u001b[39m\u001b[38;5;124m\"\u001b[39m, [pattern_n], greedy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLONGEST\u001b[39m\u001b[38;5;124m\"\u001b[39m)    \n\u001b[0;32m----> 7\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(text)\n\u001b[1;32m      8\u001b[0m matches \u001b[38;5;241m=\u001b[39m matcher(doc)\n\u001b[1;32m      9\u001b[0m matches\u001b[38;5;241m.\u001b[39msort (key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# in case that the results will be organized in descending order based on the length (given the greedy function)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "# We escape the he code above and try another method\n",
    "# the starter code for extracting the propoer nouns encounter problem when the propoer noun contains mutiple words\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_n = [{\"POS\": \"PROPN\", \"OP\": \"+\" }] # with OP it will look for propoer noun one or more times\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern_n], greedy=\"LONGEST\")    \n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort (key = lambda x: x[1])  # in case that the results will be organized in descending order based on the length (given the greedy function)\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f92f2",
   "metadata": {},
   "source": [
    "### Finding quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e50a4a-1e2b-4eda-90b5-7a4f03203b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(\"A1_grp3_txt\"): \n",
    "    print(file)\n",
    "\n",
    "    with open (\"A1_grp3_txt/\"+file, \"r\", encoding='utf-8') as f: #open all files in the folder and read them\n",
    "        text = f.read()\n",
    "    \n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    \n",
    "    pattern_q16 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #7\n",
    "    pattern_q17 = [{'ORTH': '“'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '”'}] #4\n",
    "    # both curly and straight quotes\n",
    "    \n",
    "    matcher.add(\"QUOTES\", [pattern_q16, pattern_q17])\n",
    "    doc = nlp(text)\n",
    "    matches_q = matcher(doc)\n",
    "    print(len(matches_q))\n",
    "    for match in matches_q[:10]:\n",
    "        print(match, doc[match[1]:match[2]])\n",
    "    print(\"\\n\") #blank space between outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08924137-c0da-4103-a581-aa7e075f5a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4070a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# combination of different OP. \n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_q = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"!\"}, {'IS_PUNCT': True, \"OP\": \"!\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q1 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"!\"}, {'IS_PUNCT': True, \"OP\": \"?\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q2 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"!\"}, {'IS_PUNCT': True, \"OP\": \"+\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q3 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"!\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #0\n",
    "\n",
    "pattern_q4 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"?\"}, {'IS_PUNCT': True, \"OP\": \"!\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q5 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"?\"}, {'IS_PUNCT': True, \"OP\": \"?\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q6 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"?\"}, {'IS_PUNCT': True, \"OP\": \"+\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q7 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"?\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #0\n",
    "\n",
    "pattern_q8 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"!\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q9 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"?\"}, {'ORTH': '\"'}] #3\n",
    "pattern_q10 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"+\"}, {'ORTH': '\"'}] #3\n",
    "pattern_q11 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #3\n",
    "\n",
    "pattern_q12 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"!\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q13 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"?\"}, {'ORTH': '\"'}] #3\n",
    "pattern_q14 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"+\"}, {'ORTH': '\"'}] #3\n",
    "pattern_q15 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #3\n",
    "\n",
    "pattern_q16 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #7\n",
    "pattern_q17 = [{'ORTH': '“'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '”'}] #4\n",
    "\n",
    "#pattern_q4 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'ORTH': '\"'}]  #control\n",
    "#pattern_q5 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'ORTH': '.,!?': True, \"OP\": \"+\"}, {'ORTH': '\"'}] \n",
    "#pattern_q6 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'ORTH': '\"'}]\n",
    "#pattern_q7 = [{'ORTH': '\"'}, {'IS_ALPHA': True, 'OP': '?'}, {'ORTH': '\"'}]\n",
    "\n",
    "matcher.add(\"QUOTES\", [pattern_q16, pattern_q17])\n",
    "doc = nlp(text)\n",
    "matches_q = matcher(doc)\n",
    "# matches_q.sort(key = lambda x: x[1])\n",
    "print (len(matches_q))\n",
    "for match in matches_q[:10]:\n",
    "    print (match, doc[match[1]:match[2]])\n",
    "\n",
    "# MAYBE STRAIGHT/CURLY QUOTATION MARKS MATTER, SINCE IT IS RECOGNISING THE MIDDLE PART OF THE QUOTES AS WELL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296dbcc1",
   "metadata": {},
   "source": [
    "## Approach 3: Implemented version\n",
    "This approach was implemented by colleagues at the [Australian Text Analytics Platform](https://www.atap.edu.au/) (ATAP). The approach is based on the [Gender Gap Tracker](https://github.com/sfu-discourse-lab/GenderGapTracker) done in the Discourse Processing Lab here at SFU. \n",
    "\n",
    "The first link below leads you to a binder where you can load your own files and download the output. If you prefer to do everything in your own notebook, you can download/clone the project from GitHub. \n",
    "\n",
    "* [Binder link](https://github.com/Australian-Text-Analytics-Platform/quotation-tool/blob/workshop_01_20220908/README.md)\n",
    "\n",
    "    * Click on the \"binder launch\" button.\n",
    "    * At the CILogin, under \"Select an Identity Provider\", go to the drop-down menu (usually default as ORCID) and select \"Simon Fraser University\".\n",
    "    * This launches [Binder](https://mybinder.readthedocs.io/en/latest/), a service that allows you to run a notebook online on Jupyter Lab (similar to Google Colab). \n",
    "    * Run all the code cells in that notebook, uploading files from the A1_data directory. \n",
    "    * At the end, you can save the output as an Excel file. \n",
    "\n",
    "* [Regular GitHub project](https://github.com/Australian-Text-Analytics-Platform/quotation-tool)\n",
    "\n",
    "    * Run the notebook \"quote_extractor_notebook.ipynb\"\n",
    "\n",
    "Within the ATAP binder, upload 5 files from A1_data (the same you did for approaches 1 and 2), process them and download the results to your own computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8674803d",
   "metadata": {},
   "source": [
    "## Your turn\n",
    "\n",
    "Check instructions on Canvas for what to do and what to submit. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ef8ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
