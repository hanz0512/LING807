{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be347187",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Ling 450/807 SFU - Assignment 1\n",
    "\n",
    "Virginia Uhi, Eunice Wong, & Han Zhang\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cd57cf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Import packages\n",
    "\n",
    "We import everything we will need here at the beginning and load the spaCy language model. Note that we are using the small English model. One thing you could try is to download and load [other models for English](https://spacy.io/models/en) and compare the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65927718",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d845f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Approach 1: Using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5755aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Some of those ‘soft’ skills are in short supply, but they’re what employers are looking for,', 'There’s a societal cost to having someone remain unemployed,', 'There are health-care costs. There are welfare costs. There can be criminal justice costs,', 'road map', 'should move to skills-based hiring,', 'practices that prioritize … credentials and experience.', 'These are things that are the least susceptible to technological disruption,']\n",
      "[\"It's good that we take the time early on to completely shut hockey off and just relax and then the closer we get to coming back, we've got to get the minds ready,\", \"Most guys were here last year and we learned a lesson from last year when we weren't as good after the break. We know what to do this time and what not to.\", 'He enabled us to at least hang in there and get a point out of this game,', 'The positive is you came in here against one of the best teams and in my opinion, we could have had that game.', \"I don't think we played our best game, but again, we found a way to win and I think that's a big key,\", \"That was one of our weaknesses last year. We couldn't find ways to win and this year, it's the opposite. Good teams, they find ways to win.\", \"Going into the break it's important to go in with a good taste in your mouth,\", \"We played the best when the game was on the line in the third and I thought we had some great looks that made Mrazek make some big saves for them. ... We'll take the two points heading into the break and we'll come back refreshed and ready to go.\"]\n",
      "[\"As a young person, I've experienced that fear and distrust. Just seeing police officers, I feel like I can be also racially targeted,\", \"To avoid any cases like that in the future, it's important to come together and to strategize and to be honest,\", \"What gives me hope is the level of commitment that our police chief has to working alongside FSIN youth because there hasn't been a relationship historically between the FSIN youth representatives and Saskatoon city police,\", 'I thought this was a good time to write a letter to the Saskatoon city police on questioning [if they are] going to be taking body cameras on,', \"And if so, could they kind of explain to FSIN youth what it's gonna look like and what the timeline is.\", 'very committed', 'life-changing and heartbreaking,', \"I can honestly say that it's very emotional to listen to stories of the kind of suffering that goes on in the Indigenous community due to being impoverished, due to violence and due to racism,\", \"The treatment of Indigenous peoples in this country, it's heartbreaking but I can also say that it provides me hope as well when I talk to young people and I see that they persevere.\", \"It may not look perfect to other people, but I can say that I've always been authentically myself I would want all young people to do the same,\", 'I would say there definitely is pressure, but I try to be as gentle and understanding with myself as I would expect to how I should treat another young person.', 'As young people we understand that this land is foundational to who we are,']\n",
      "['I’m so excited and I am so honoured to be selected by you here,', 'My eyes are full of tears because I love this land so deeply,', 'He’s not from our local community. He cannot represent you, represent us,', 'I believe we will have a good chance to win in Burnaby South as I believe right now I’m very familiar with our community. I’m one of the people here.', 'elected', 'Essentially, the leader will choose the candidate in each byelection,', 'whole country']\n",
      "['The Australian Government is concerned about the recent detention of two Canadian citizens in China,', 'We would be very concerned if these cases were related to legal proceedings currently underway in Canada involving a Chinese citizen, Ms. Meng Wanzhou.', 'basically kidnapping', 'every confidence in the fairness and independence of Canada’s administration of justice.', 'We are deeply concerned about the recent detentions of Canadian citizens Michael Kovrig and Michael Spavor in the People’s Republic of China. We ask the Australian Government without further delay to support Canada’s call for the immediate release of these two detainees,', 'deep concern', 'We share Canada’s commitment to the rule of law as fundamental to all free societies, and we will defend and uphold this principle,', 'the declared motive for their investigation raises concerns about legitimate research and business practices in China.', 'The denial of access to a lawyer under their status of detention is contrary to the right of defence. The EU fully supports the efforts of the Canadian Government on this matter.']\n"
     ]
    }
   ],
   "source": [
    "# The starter file loads and processes only one file at a time. We are using the command to do 5 files at the same time\n",
    "\n",
    "# First we take five text files out \n",
    "\n",
    "five_files = [\"A1_data/5c1548a31e67d78e2771624f.txt\", \"A1_data/5c489df91e67d78e271d66c5.txt\", \"A1_data/5c182ac21e67d78e277944ad.txt\", \"A1_data/5c28972a795bd2fac69fa974.txt\", \"A1_data/5c29beda1e67d78e27b74939.txt\"]\n",
    "\n",
    "# Creat a file\n",
    "file_path = \"Regular_Expression_All.txt\"\n",
    "with open(file_path, 'w', encoding='utf-8'):\n",
    "    pass\n",
    "\n",
    "# Process five text files at the same time, and have all the quotes in the five files can be extracted and put in the same file\n",
    "with open (\"Regular_Expression_All.txt\", 'w', encoding = \"utf-8\") as output:\n",
    "    def get_quotes(text):\n",
    "        quotes = re.findall(r'[\"“](.*?)[”\"]', text) \n",
    "            # so that every time as soon as Spacy identify any quotation marks (either straight or curly), the quoted content with the quotation mark will be extracted\n",
    "        return(quotes)\n",
    "    \n",
    "    for file_path in five_files: \n",
    "        output.write(file_path + \"\\n\")\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "                # The starter code seperate the text into sentences at the begining; However, for the sake of better identifying the sentence, we decide to not seperate the sentences first\n",
    "\n",
    "            found_quotes = get_quotes(text) \n",
    "            print(found_quotes)\n",
    "            for quote in found_quotes:\n",
    "                output.write(str(quote) + \"\\n\")  # Using the \"\\n\" to make the quotes listed line by line\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25227db3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Approach 2: Using spaCy's Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b207ae63",
   "metadata": {},
   "source": [
    "This approach is based on notebooks by [William J.B. Mattingly](https://wjbmattingly.com/). His book, Introduction to Python for Humanists, is available online from the [SFU Library](https://sfu-primo.hosted.exlibrisgroup.com/permalink/f/usv8m3/01SFUL_ALMA51476999620003611). \n",
    "\n",
    "For more on spaCy's Matcher, see Advanced NLP with spaCy, [chapter 2](https://course.spacy.io/en/chapter2)). \n",
    "\n",
    "We have already loaded everything we need at the beginning of this notebook (imported Matcher, assigned it to a `matcher` object), so now we can use it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f0879",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Finding quotes and speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c4213f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert it to a spacy doc\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ee2f7c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Finding proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f78b579d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "#This is optional. It just tells you who are the people mentioned. You can use it later if you want to find out the speakers of the quotes\n",
    "\n",
    "#matcher = Matcher(nlp.vocab)\n",
    "#pattern_n = [{\"POS\": \"PROPN\"}]\n",
    "#matcher.add(\"PROPER_NOUNS\", [pattern_n], greedy=\"LONGEST\")\n",
    "#doc = nlp(text)\n",
    "#matches = matcher(doc)\n",
    "#print (len(matches))\n",
    "#for match in matches[:10]:\n",
    "    #print (match, doc[match[1]:match[2]])\n",
    "    \n",
    "##You can try to extract full names by adding multi-word nouns, http://spacy.pythonhumanities.com/02_02_matcher.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "406f596a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "(3232560085755078826, 0, 1) Australia\n",
      "(3232560085755078826, 9, 10) Canadians\n",
      "(3232560085755078826, 12, 13) China\n",
      "(3232560085755078826, 30, 31) Canada\n",
      "(3232560085755078826, 39, 44) Foreign Affairs Minister Marise Payne\n",
      "(3232560085755078826, 45, 46) Sunday\n",
      "(3232560085755078826, 55, 56) Australia\n",
      "(3232560085755078826, 71, 72) France\n",
      "(3232560085755078826, 73, 76) New York Times\n",
      "(3232560085755078826, 83, 84) China\n"
     ]
    }
   ],
   "source": [
    "##We skip the he code above and try another method\n",
    "#the starter code for extracting the propoer nouns encounter problem when the propoer noun contains mutiple words\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_n = [{\"POS\": \"PROPN\", \"OP\": \"+\" }] # with OP it will look for propoer noun one or more times\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern_n], greedy=\"LONGEST\")    \n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort (key = lambda x: x[1])  # in case that the results will be organized in descending order based on the length (given the greedy function)\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a980549-f349-47c6-9f6d-83565a9ed678",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Finding quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0127acd-6569-42ff-ad1b-7a309b048307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "(16432004385153140588, 397, 401) “road map”\n",
      "(16432004385153140588, 413, 423) “should move to skills-based hiring,”\n",
      "(16432004385153140588, 426, 436) “practices that prioritize … credentials and experience.”\n",
      "(16432004385153140588, 628, 642) “These are things that are the least susceptible to technological disruption,”\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#text file 1\n",
    "with open (\"A1_data/5c1548a31e67d78e2771624f.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    \n",
    "    pattern_q16 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #7\n",
    "    pattern_q17 = [{'ORTH': '“'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '”'}] #4\n",
    "    # both curly and straight quotes\n",
    "    \n",
    "    matcher.add(\"QUOTES\", [pattern_q16, pattern_q17])\n",
    "    doc = nlp(text)\n",
    "    matches_q = matcher(doc)\n",
    "    print(len(matches_q))\n",
    "    for match in matches_q[:10]:\n",
    "        print(match, doc[match[1]:match[2]])\n",
    "    print(\"\\n\") #blank space between outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4ce26e8-5dbb-4432-b49c-6edef9d9bd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(16432004385153140588, 331, 351) \"He enabled us to at least hang in there and get a point out of this game,\"\n",
      "(16432004385153140588, 358, 385) \"The positive is you came in here against one of the best teams and in my opinion, we could have had that game.\"\n",
      "(16432004385153140588, 706, 711) \" Peters said. \"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#text file 2\n",
    "with open (\"A1_data/5c489df91e67d78e271d66c5.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    \n",
    "    pattern_q16 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #7\n",
    "    pattern_q17 = [{'ORTH': '“'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '”'}]\n",
    "    # both curly and straight quotes\n",
    "    \n",
    "    matcher.add(\"QUOTES\", [pattern_q16, pattern_q17])\n",
    "    doc = nlp(text)\n",
    "    matches_q = matcher(doc)\n",
    "    print(len(matches_q))\n",
    "    for match in matches_q[:10]:\n",
    "        print(match, doc[match[1]:match[2]])\n",
    "    print(\"\\n\") #blank space between outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efae1918-d4fd-43e3-b7a9-950723c9e507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "(16432004385153140588, 366, 370) \"very committed\"\n",
      "(16432004385153140588, 445, 453) \"life-changing and heartbreaking,\"\n",
      "(16432004385153140588, 596, 630) \"I would say there definitely is pressure, but I try to be as gentle and understanding with myself as I would expect to how I should treat another young person.\"\n",
      "(16432004385153140588, 677, 694) \"As young people we understand that this land is foundational to who we are,\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#text file 3\n",
    "with open (\"A1_data/5c182ac21e67d78e277944ad.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    \n",
    "    pattern_q16 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #7\n",
    "    pattern_q17 = [{'ORTH': '“'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '”'}] #4\n",
    "    # both curly and straight quotes\n",
    "    \n",
    "    matcher.add(\"QUOTES\", [pattern_q16, pattern_q17])\n",
    "    doc = nlp(text)\n",
    "    matches_q = matcher(doc)\n",
    "    print(len(matches_q))\n",
    "    for match in matches_q[:10]:\n",
    "        print(match, doc[match[1]:match[2]])\n",
    "    print(\"\\n\") #blank space between outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3721dfa-803b-4f7d-bf7e-d1fdffd5df44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(16432004385153140588, 128, 146) “The Australian Government is concerned about the recent detention of two Canadian citizens in China,”\n",
      "(16432004385153140588, 265, 269) “basically kidnapping”\n",
      "(16432004385153140588, 514, 518) “deep concern”\n",
      "(16432004385153140588, 566, 585) “the declared motive for their investigation raises concerns about legitimate research and business practices in China.”\n",
      "(16432004385153140588, 587, 623) “The denial of access to a lawyer under their status of detention is contrary to the right of defence. The EU fully supports the efforts of the Canadian Government on this matter.”\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#text file 4\n",
    "with open (\"A1_data/5c29beda1e67d78e27b74939.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    \n",
    "    pattern_q16 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #7\n",
    "    pattern_q17 = [{'ORTH': '“'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '”'}] #4\n",
    "    # both curly and straight quotes\n",
    "    \n",
    "    matcher.add(\"QUOTES\", [pattern_q16, pattern_q17])\n",
    "    doc = nlp(text)\n",
    "    matches_q = matcher(doc)\n",
    "    print(len(matches_q))\n",
    "    for match in matches_q[:10]:\n",
    "        print(match, doc[match[1]:match[2]])\n",
    "    print(\"\\n\") #blank space between outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e91c779d-df8d-4da6-b824-f6273b58d4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(16432004385153140588, 128, 144) “My eyes are full of tears because I love this land so deeply,”\n",
      "(16432004385153140588, 576, 590) “Essentially, the leader will choose the candidate in each byelection,”\n",
      "(16432004385153140588, 619, 623) “whole country”\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#text file 5\n",
    "with open (\"A1_data/5c28972a795bd2fac69fa974.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    \n",
    "    pattern_q16 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #7\n",
    "    pattern_q17 = [{'ORTH': '“'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '”'}] #4\n",
    "    # both curly and straight quotes\n",
    "    \n",
    "    matcher.add(\"QUOTES\", [pattern_q16, pattern_q17])\n",
    "    doc = nlp(text)\n",
    "    matches_q = matcher(doc)\n",
    "    print(len(matches_q))\n",
    "    for match in matches_q[:10]:\n",
    "        print(match, doc[match[1]:match[2]])\n",
    "    print(\"\\n\") #blank space between outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c4070a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(16432004385153140588, 128, 144) “My eyes are full of tears because I love this land so deeply,”\n",
      "(16432004385153140588, 576, 590) “Essentially, the leader will choose the candidate in each byelection,”\n",
      "(16432004385153140588, 619, 623) “whole country”\n"
     ]
    }
   ],
   "source": [
    "# combination of different OP. \n",
    "# some trials and errors\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_q = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"!\"}, {'IS_PUNCT': True, \"OP\": \"!\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q1 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"!\"}, {'IS_PUNCT': True, \"OP\": \"?\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q2 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"!\"}, {'IS_PUNCT': True, \"OP\": \"+\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q3 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"!\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #0\n",
    "\n",
    "pattern_q4 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"?\"}, {'IS_PUNCT': True, \"OP\": \"!\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q5 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"?\"}, {'IS_PUNCT': True, \"OP\": \"?\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q6 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"?\"}, {'IS_PUNCT': True, \"OP\": \"+\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q7 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"?\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #0\n",
    "\n",
    "pattern_q8 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"!\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q9 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"?\"}, {'ORTH': '\"'}] #3\n",
    "pattern_q10 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"+\"}, {'ORTH': '\"'}] #3\n",
    "pattern_q11 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #3\n",
    "\n",
    "pattern_q12 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"!\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q13 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"?\"}, {'ORTH': '\"'}] #3\n",
    "pattern_q14 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"+\"}, {'ORTH': '\"'}] #3\n",
    "pattern_q15 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #3\n",
    "\n",
    "pattern_q16 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #7\n",
    "pattern_q17 = [{'ORTH': '“'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '”'}] #4\n",
    "\n",
    "#pattern_q4 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'ORTH': '\"'}]  #control\n",
    "#pattern_q5 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'ORTH': '.,!?': True, \"OP\": \"+\"}, {'ORTH': '\"'}] \n",
    "#pattern_q6 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'ORTH': '\"'}]\n",
    "#pattern_q7 = [{'ORTH': '\"'}, {'IS_ALPHA': True, 'OP': '?'}, {'ORTH': '\"'}]\n",
    "\n",
    "matcher.add(\"QUOTES\", [pattern_q16, pattern_q17])\n",
    "doc = nlp(text)\n",
    "matches_q = matcher(doc)\n",
    "# matches_q.sort(key = lambda x: x[1])\n",
    "print (len(matches_q))\n",
    "for match in matches_q[:10]:\n",
    "    print (match, doc[match[1]:match[2]])\n",
    "\n",
    "# MAYBE STRAIGHT/CURLY QUOTATION MARKS MATTER, SINCE IT IS RECOGNISING THE MIDDLE PART OF THE QUOTES AS WELL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296dbcc1",
   "metadata": {},
   "source": [
    "# Approach 3: Implemented version\n",
    "This approach was implemented by colleagues at the [Australian Text Analytics Platform](https://www.atap.edu.au/) (ATAP). The approach is based on the [Gender Gap Tracker](https://github.com/sfu-discourse-lab/GenderGapTracker) done in the Discourse Processing Lab here at SFU. \n",
    "\n",
    "The first link below leads you to a binder where you can load your own files and download the output. If you prefer to do everything in your own notebook, you can download/clone the project from GitHub. \n",
    "\n",
    "* [Binder link](https://github.com/Australian-Text-Analytics-Platform/quotation-tool/blob/workshop_01_20220908/README.md)\n",
    "\n",
    "    * Click on the \"binder launch\" button.\n",
    "    * At the CILogin, under \"Select an Identity Provider\", go to the drop-down menu (usually default as ORCID) and select \"Simon Fraser University\".\n",
    "    * This launches [Binder](https://mybinder.readthedocs.io/en/latest/), a service that allows you to run a notebook online on Jupyter Lab (similar to Google Colab). \n",
    "    * Run all the code cells in that notebook, uploading files from the A1_data directory. \n",
    "    * At the end, you can save the output as an Excel file. \n",
    "\n",
    "* [Regular GitHub project](https://github.com/Australian-Text-Analytics-Platform/quotation-tool)\n",
    "\n",
    "    * Run the notebook \"quote_extractor_notebook.ipynb\"\n",
    "\n",
    "Within the ATAP binder, upload 5 files from A1_data (the same you did for approaches 1 and 2), process them and download the results to your own computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8674803d",
   "metadata": {},
   "source": [
    "## Your turn\n",
    "\n",
    "Check instructions on Canvas for what to do and what to submit. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ef8ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
