{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be347187",
   "metadata": {},
   "source": [
    "# Ling 450/807 SFU - Assignment 1\n",
    "\n",
    "This assignment walks you through two different ways of extracting simple quotes from text and then directs you to a third, already implemented way. Your task is to enhance the simple methods or develop your own. For further instructions, check the assignment file on Canvas. You'll need this notebook and the sample files in the A1_data directory. \n",
    "\n",
    "Group 3: Virginia Uhi, Eunice Wong, & Han Zhang\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cd57cf",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "\n",
    "We import everything we will need here at the beginning and load the spaCy language model. Note that we are using the small English model. One thing you could try is to download and load [other models for English](https://spacy.io/models/en) and compare the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65927718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Spacy \n",
    "import spacy\n",
    "import re\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d845f6",
   "metadata": {},
   "source": [
    "## Approach 1: Using regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3e3af6",
   "metadata": {},
   "source": [
    "### Text 1: 5c1548a31e67d78e2771624f.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f2bcdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this loads and processes only one file at a time. You need to do 5 and comment on the results\n",
    "# to load the 5 texts, you can just change the name of the file below or figure out a way \n",
    "# to pass a list of files to the read command. It's up to you\n",
    "\n",
    "# Loading file 1\n",
    "with open (\"A1_data/5c1548a31e67d78e2771624f.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5755aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sents(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    return(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8218a1e",
   "metadata": {},
   "source": [
    "### Finding text within quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b26f91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quotes(text):\n",
    "    quotes = re.findall(r'[\"“](.*?)[”\"]', text)\n",
    "    return(quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10d48013",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_sents = find_sents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e648a947",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Some of those ‘soft’ skills are in short supply, but they’re what employers are looking for,']\n",
      "['There’s a societal cost to having someone remain unemployed,']\n",
      "['road map', 'should move to skills-based hiring,', 'practices that prioritize … credentials and experience.']\n",
      "['These are things that are the least susceptible to technological disruption,']\n"
     ]
    }
   ],
   "source": [
    "# note: this just prints the text in quotes. If you want to save it locally\n",
    "# to analyze how the 3 approaches are different, you need to run a command to save\n",
    "# to a text file\n",
    "\n",
    "file = open (\"Assignment1_Output1.txt\", 'w')\n",
    "\n",
    "for sent in found_sents:\n",
    "    str_sent = str(sent)\n",
    "    found_quotes = get_quotes(str_sent)\n",
    "    if len(found_quotes) > 0:\n",
    "        print(found_quotes)\n",
    "    \n",
    "\n",
    "#    if len(found_quotes1) > 0:\n",
    "#       print(found_quotes1)\n",
    "#        file.write(str(found_quotes1) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804eb2e2",
   "metadata": {},
   "source": [
    "### Text 2: 5c489df91e67d78e271d66c5.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3ee23b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading file 2\n",
    "with open (\"A1_data/5c489df91e67d78e271d66c5.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b115576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sents(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    return(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b33e95",
   "metadata": {},
   "source": [
    "### Finding text within quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12c83e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quotes(text):\n",
    "    quotes = re.findall(r'[\"“](.*?)[”\"]', text)\n",
    "    return(quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0a7e8315",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_sents = find_sents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "068d290e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It's good that we take the time early on to completely shut hockey off and just relax and then the closer we get to coming back, we've got to get the minds ready,\"]\n",
      "['He enabled us to at least hang in there and get a point out of this game,']\n",
      "['The positive is you came in here against one of the best teams and in my opinion, we could have had that game.']\n",
      "[\"I don't think we played our best game, but again, we found a way to win and I think that's a big key,\"]\n",
      "[\"Going into the break it's important to go in with a good taste in your mouth,\"]\n"
     ]
    }
   ],
   "source": [
    "# note: this just prints the text in quotes. If you want to save it locally\n",
    "# to analyze how the 3 approaches are different, you need to run a command to save\n",
    "# to a text file\n",
    "\n",
    "file2 = open (\"Assignment1_Output2.txt\", 'w')\n",
    "\n",
    "for sent in found_sents:\n",
    "    str_sent = str(sent)\n",
    "    found_quotes = get_quotes(str_sent)\n",
    "    if len(found_quotes) > 0:\n",
    "        print(found_quotes)\n",
    "    \n",
    "\n",
    "#    if len(found_quotes1) > 0:\n",
    "#       print(found_quotes1)\n",
    "#        file.write(str(found_quotes1) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd9f251",
   "metadata": {},
   "source": [
    "### Text 3: 5c182ac21e67d78e277944ad.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e182da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading file 3\n",
    "with open (\"A1_data/5c182ac21e67d78e277944ad.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7197b24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sents(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    return(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac0a765",
   "metadata": {},
   "source": [
    "### Finding text within quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "104a5118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quotes(text):\n",
    "    quotes = re.findall(r'[\"“](.*?)[”\"]', text)\n",
    "    return(quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "49874a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_sents = find_sents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "38b1b7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"To avoid any cases like that in the future, it's important to come together and to strategize and to be honest,\"]\n",
      "[\"What gives me hope is the level of commitment that our police chief has to working alongside FSIN youth because there hasn't been a relationship historically between the FSIN youth representatives and Saskatoon city police,\"]\n",
      "['I thought this was a good time to write a letter to the Saskatoon city police on questioning [if they are] going to be taking body cameras on,']\n",
      "[\"And if so, could they kind of explain to FSIN youth what it's gonna look like and what the timeline is.\"]\n",
      "['very committed']\n",
      "['life-changing and heartbreaking,']\n",
      "[\"I can honestly say that it's very emotional to listen to stories of the kind of suffering that goes on in the Indigenous community due to being impoverished, due to violence and due to racism,\"]\n",
      "[\"The treatment of Indigenous peoples in this country, it's heartbreaking but I can also say that it provides me hope as well when I talk to young people and I see that they persevere.\"]\n",
      "[\"It may not look perfect to other people, but I can say that I've always been authentically myself I would want all young people to do the same,\"]\n",
      "['I would say there definitely is pressure, but I try to be as gentle and understanding with myself as I would expect to how I should treat another young person.']\n",
      "['As young people we understand that this land is foundational to who we are,']\n"
     ]
    }
   ],
   "source": [
    "file3 = open (\"Assignment1_Output3.txt\", 'w')\n",
    "\n",
    "for sent in found_sents:\n",
    "    str_sent = str(sent)\n",
    "    found_quotes = get_quotes(str_sent)\n",
    "    if len(found_quotes) > 0:\n",
    "        print(found_quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00210c5f",
   "metadata": {},
   "source": [
    "### Text 4: 5c28972a795bd2fac69fa974.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1aefc983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading file 4\n",
    "with open (\"A1_data/5c28972a795bd2fac69fa974.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ff7a9375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sents(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    return(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f441de56",
   "metadata": {},
   "source": [
    "### Finding text within quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "09a76c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quotes(text):\n",
    "    quotes = re.findall(r'[\"“](.*?)[”\"]', text)\n",
    "    return(quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4e866c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_sents = find_sents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d4ef0016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I’m so excited and I am so honoured to be selected by you here,']\n",
      "['My eyes are full of tears because I love this land so deeply,']\n",
      "['elected']\n",
      "['Essentially, the leader will choose the candidate in each byelection,']\n",
      "['whole country']\n"
     ]
    }
   ],
   "source": [
    "file4 = open (\"Assignment1_Output4.txt\", 'w')\n",
    "\n",
    "for sent in found_sents:\n",
    "    str_sent = str(sent)\n",
    "    found_quotes = get_quotes(str_sent)\n",
    "    if len(found_quotes) > 0:\n",
    "        print(found_quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db67c2b",
   "metadata": {},
   "source": [
    "### Text 5: 5c29beda1e67d78e27b74939.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9120ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading file 5\n",
    "with open (\"A1_data/5c29beda1e67d78e27b74939.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c26f7605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sents(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    return(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419abdff",
   "metadata": {},
   "source": [
    "### Finding text within quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f4b92c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quotes(text):\n",
    "    quotes = re.findall(r'[\"“](.*?)[”\"]', text)\n",
    "    return(quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9ce8c2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_sents = find_sents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3401640d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Australian Government is concerned about the recent detention of two Canadian citizens in China,']\n",
      "['We would be very concerned if these cases were related to legal proceedings currently underway in Canada involving a Chinese citizen, Ms. Meng Wanzhou.']\n",
      "['basically kidnapping']\n",
      "['every confidence in the fairness and independence of Canada’s administration of justice.']\n",
      "['deep concern']\n",
      "['We share Canada’s commitment to the rule of law as fundamental to all free societies, and we will defend and uphold this principle,']\n",
      "['the declared motive for their investigation raises concerns about legitimate research and business practices in China.']\n"
     ]
    }
   ],
   "source": [
    "file5 = open (\"Assignment1_Output5.txt\", 'w')\n",
    "\n",
    "for sent in found_sents:\n",
    "    str_sent = str(sent)\n",
    "    found_quotes = get_quotes(str_sent)\n",
    "    if len(found_quotes) > 0:\n",
    "        print(found_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3153bbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempts For Text1\n",
    "\n",
    "#with open (\"A1_data/5c5d3f0a1e67d78e275e5788.txt\", \"r\", encoding='utf-8') as f:\n",
    "  #  text2 = f.read() \n",
    "    \n",
    "#def find_sents(text):\n",
    "    #doc = nlp(text)\n",
    "    #sentences = list(doc.sents)\n",
    "    #return(sentences)\n",
    "\n",
    "#def get_quotes(text):\n",
    "    #quotes = re.findall(r' [“\"](.*?)[\"”]', text)\n",
    "    #return(quotes)\n",
    "\n",
    "#found_sents2 = find_sents(text2)\n",
    "\n",
    "#file = open(\"Assignment1_Output2.txt\", 'w')\n",
    "#for sent in found_sents2:\n",
    "   # str_sent = str(sent)\n",
    "    #found_quotes2 = get_quotes(str_sent)\n",
    "\n",
    "   # if len(found_quotes2) > 0:\n",
    "       # print(found_quotes2)\n",
    "       # file.write(str(found_quotes2) + \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25227db3",
   "metadata": {},
   "source": [
    "## Approach 2: Using spaCy's Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b207ae63",
   "metadata": {},
   "source": [
    "This approach is based on notebooks by [William J.B. Mattingly](https://wjbmattingly.com/). His book, Introduction to Python for Humanists, is available online from the [SFU Library](https://sfu-primo.hosted.exlibrisgroup.com/permalink/f/usv8m3/01SFUL_ALMA51476999620003611). \n",
    "\n",
    "For more on spaCy's Matcher, see Advanced NLP with spaCy, [chapter 2](https://course.spacy.io/en/chapter2)). \n",
    "\n",
    "We have already loaded everything we need at the beginning of this notebook (imported Matcher, assigned it to a `matcher` object), so now we can use it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f0879",
   "metadata": {},
   "source": [
    "## Finding quotes and speakers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8912812",
   "metadata": {},
   "source": [
    "### Text file 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "033dc122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text file 1. \n",
    "#Remember, you have to do 5\n",
    "\n",
    "with open (\"A1_data/5c1548a31e67d78e2771624f.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3c4213f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert it to a spacy doc\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ee2f7c",
   "metadata": {},
   "source": [
    "### Finding proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cd311bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(3232560085755078826, 68, 71) Andrew Francis Wallace\n",
      "(3232560085755078826, 72, 75) Toronto Star File\n"
     ]
    }
   ],
   "source": [
    "# This is optional. It just tells you who are the people mentioned. You can use it later if you want to find out the speakers of the quotes\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "#for token in doc:\n",
    "    #if token.pos_== 'PROPN'\n",
    "    \n",
    "pattern_n = [{\"POS\": \"PROPN\"},{\"POS\": \"PROPN\"},{\"POS\": \"PROPN\"}]\n",
    "\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern_n], greedy=\"LONGEST\")\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])\n",
    "    \n",
    "## You can try to extract full names by adding multi-word nouns, http://spacy.pythonhumanities.com/02_02_matcher.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f92f2",
   "metadata": {},
   "source": [
    "### Finding quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3c4070a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ORTH': '\"'}, {'IS_ALPHA': True, 'OP': '+'}, {'ORTH': '\"'}]\n",
      "2\n",
      "(16432004385153140588, 397, 401) “road map”\n",
      "(16432004385153140588, 628, 642) “These are things that are the least susceptible to technological disruption,”\n"
     ]
    }
   ],
   "source": [
    "# a simple pattern to extract things in single quotes\n",
    "# as with Approach 1, the for loop prints the results to the screen\n",
    "# you can try and save it to a file if you want to compare with Approach 1 and 3\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern_q = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"},{'ORTH': '\"'}]\n",
    "pattern_q1 = [{'ORTH': '\"'}, {'IS_PUNCT': True, \"OP\": \"+\"},{' ': True, \"OP\": \"+\"},{'ORTH': '\"'}]\n",
    "\n",
    "print(pattern_q)\n",
    "\n",
    "pattern_q = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}]\n",
    "pattern_q1 = [{'ORTH': '“'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '”'}]\n",
    "matcher.add(\"QUOTES\", [pattern_q, pattern_q1], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches_q = matcher(doc)\n",
    "matches_q.sort(key = lambda x: x[1])\n",
    "print (len(matches_q))\n",
    "for match in matches_q[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5153bc6",
   "metadata": {},
   "source": [
    "### Text file 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "09e44ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text file 2. \n",
    "with open (\"A1_data/5c489df91e67d78e271d66c5.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "820eeafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert it to a spacy doc\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "22504fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Finding proper nouns\n",
    "# This is optional. It just tells you who are the people mentioned. You can use it later if you want to find out the speakers of the quotes\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "#for token in doc:\n",
    "    #if token.pos_== 'PROPN'\n",
    "    \n",
    "pattern_n = [{\"POS\": \"PROPN\"},{\"POS\": \"PROPN\"},{\"POS\": \"PROPN\"}]\n",
    "\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern_n], greedy=\"LONGEST\")\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])\n",
    "    \n",
    "## You can try to extract full names by adding multi-word nouns, http://spacy.pythonhumanities.com/02_02_matcher.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "35476150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ORTH': '\"'}, {'IS_ALPHA': True, 'OP': '+'}, {'ORTH': '\"'}]\n",
      "2\n",
      "(16432004385153140588, 331, 351) \"He enabled us to at least hang in there and get a point out of this game,\"\n",
      "(16432004385153140588, 706, 711) \" Peters said. \"\n"
     ]
    }
   ],
   "source": [
    "# Finding quotes\n",
    "\n",
    "# a simple pattern to extract things in single quotes\n",
    "# as with Approach 1, the for loop prints the results to the screen\n",
    "# you can try and save it to a file if you want to compare with Approach 1 and 3\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern_q = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"},{'ORTH': '\"'}]\n",
    "pattern_q1 = [{'ORTH': '\"'}, {'IS_PUNCT': True, \"OP\": \"+\"},{' ': True, \"OP\": \"+\"},{'ORTH': '\"'}]\n",
    "\n",
    "print(pattern_q)\n",
    "\n",
    "pattern_q = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}]\n",
    "pattern_q1 = [{'ORTH': '“'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '”'}]\n",
    "matcher.add(\"QUOTES\", [pattern_q, pattern_q1], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches_q = matcher(doc)\n",
    "matches_q.sort(key = lambda x: x[1])\n",
    "print (len(matches_q))\n",
    "for match in matches_q[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75a9d35",
   "metadata": {},
   "source": [
    "### Text file 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "24406382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text file 3. \n",
    "with open (\"A1_data/5c182ac21e67d78e277944ad.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "12007f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert it to a spacy doc\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d3b98cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(3232560085755078826, 33, 36) Sovereign Indigenous Nations\n",
      "(3232560085755078826, 50, 53) Indigenous Relations Carolyn\n",
      "(3232560085755078826, 58, 61) Saskatoon Police Service\n",
      "(3232560085755078826, 394, 397) White Bear First\n",
      "(3232560085755078826, 663, 666) AFN Youth Council\n"
     ]
    }
   ],
   "source": [
    "# Finding proper nouns\n",
    "# This is optional. It just tells you who are the people mentioned. You can use it later if you want to find out the speakers of the quotes\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "#for token in doc:\n",
    "    #if token.pos_== 'PROPN'\n",
    "    \n",
    "pattern_n = [{\"POS\": \"PROPN\"},{\"POS\": \"PROPN\"},{\"POS\": \"PROPN\"}]\n",
    "\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern_n], greedy=\"LONGEST\")\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])\n",
    "    \n",
    "## You can try to extract full names by adding multi-word nouns, http://spacy.pythonhumanities.com/02_02_matcher.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e404a071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ORTH': '\"'}, {'IS_ALPHA': True, 'OP': '+'}, {'ORTH': '\"'}]\n",
      "2\n",
      "(16432004385153140588, 366, 370) \"very committed\"\n",
      "(16432004385153140588, 677, 694) \"As young people we understand that this land is foundational to who we are,\"\n"
     ]
    }
   ],
   "source": [
    "# Finding quotes\n",
    "\n",
    "# a simple pattern to extract things in single quotes\n",
    "# as with Approach 1, the for loop prints the results to the screen\n",
    "# you can try and save it to a file if you want to compare with Approach 1 and 3\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern_q = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"},{'ORTH': '\"'}]\n",
    "pattern_q1 = [{'ORTH': '\"'}, {'IS_PUNCT': True, \"OP\": \"+\"},{' ': True, \"OP\": \"+\"},{'ORTH': '\"'}]\n",
    "\n",
    "print(pattern_q)\n",
    "\n",
    "pattern_q = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}]\n",
    "pattern_q1 = [{'ORTH': '“'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '”'}]\n",
    "matcher.add(\"QUOTES\", [pattern_q, pattern_q1], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches_q = matcher(doc)\n",
    "matches_q.sort(key = lambda x: x[1])\n",
    "print (len(matches_q))\n",
    "for match in matches_q[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9a487a",
   "metadata": {},
   "source": [
    "### Text file 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bcae41b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text file 4. \n",
    "with open (\"A1_data/5c28972a795bd2fac69fa974.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "72dc2b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert it to a spacy doc\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "34ae550a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(3232560085755078826, 268, 271) Prime Minister Justin\n",
      "(3232560085755078826, 290, 293) New Democrat MP\n"
     ]
    }
   ],
   "source": [
    "# Finding proper nouns\n",
    "# This is optional. It just tells you who are the people mentioned. You can use it later if you want to find out the speakers of the quotes\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "#for token in doc:\n",
    "    #if token.pos_== 'PROPN'\n",
    "    \n",
    "pattern_n = [{\"POS\": \"PROPN\"},{\"POS\": \"PROPN\"},{\"POS\": \"PROPN\"}]\n",
    "\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern_n], greedy=\"LONGEST\")\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])\n",
    "    \n",
    "## You can try to extract full names by adding multi-word nouns, http://spacy.pythonhumanities.com/02_02_matcher.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f7869ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ORTH': '\"'}, {'IS_ALPHA': True, 'OP': '+'}, {'ORTH': '\"'}]\n",
      "3\n",
      "(16432004385153140588, 128, 144) “My eyes are full of tears because I love this land so deeply,”\n",
      "(16432004385153140588, 412, 415) “elected”\n",
      "(16432004385153140588, 619, 623) “whole country”\n"
     ]
    }
   ],
   "source": [
    "# Finding quotes\n",
    "\n",
    "# a simple pattern to extract things in single quotes\n",
    "# as with Approach 1, the for loop prints the results to the screen\n",
    "# you can try and save it to a file if you want to compare with Approach 1 and 3\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern_q = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"},{'ORTH': '\"'}]\n",
    "pattern_q1 = [{'ORTH': '\"'}, {'IS_PUNCT': True, \"OP\": \"+\"},{' ': True, \"OP\": \"+\"},{'ORTH': '\"'}]\n",
    "\n",
    "print(pattern_q)\n",
    "\n",
    "pattern_q = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}]\n",
    "pattern_q1 = [{'ORTH': '“'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '”'}]\n",
    "matcher.add(\"QUOTES\", [pattern_q, pattern_q1], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches_q = matcher(doc)\n",
    "matches_q.sort(key = lambda x: x[1])\n",
    "print (len(matches_q))\n",
    "for match in matches_q[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2315743",
   "metadata": {},
   "source": [
    "### Text file 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "860b6aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text file 5. \n",
    "with open (\"A1_data/5c29beda1e67d78e27b74939.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "937e5fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert it to a spacy doc\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "822e7206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(3232560085755078826, 39, 42) Foreign Affairs Minister\n",
      "(3232560085755078826, 73, 76) New York Times\n",
      "(3232560085755078826, 172, 175) Ms. Meng Wanzhou\n",
      "(3232560085755078826, 500, 503) U.S. State Department\n",
      "(3232560085755078826, 639, 642) Foreign Minister Marise\n"
     ]
    }
   ],
   "source": [
    "# Finding proper nouns\n",
    "# This is optional. It just tells you who are the people mentioned. You can use it later if you want to find out the speakers of the quotes\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "#for token in doc:\n",
    "    #if token.pos_== 'PROPN'\n",
    "    \n",
    "pattern_n = [{\"POS\": \"PROPN\"},{\"POS\": \"PROPN\"},{\"POS\": \"PROPN\"}]\n",
    "\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern_n], greedy=\"LONGEST\")\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])\n",
    "    \n",
    "## You can try to extract full names by adding multi-word nouns, http://spacy.pythonhumanities.com/02_02_matcher.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "71f39931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ORTH': '\"'}, {'IS_ALPHA': True, 'OP': '+'}, {'ORTH': '\"'}]\n",
      "4\n",
      "(16432004385153140588, 128, 146) “The Australian Government is concerned about the recent detention of two Canadian citizens in China,”\n",
      "(16432004385153140588, 265, 269) “basically kidnapping”\n",
      "(16432004385153140588, 514, 518) “deep concern”\n",
      "(16432004385153140588, 566, 585) “the declared motive for their investigation raises concerns about legitimate research and business practices in China.”\n"
     ]
    }
   ],
   "source": [
    "# Finding quotes\n",
    "\n",
    "# a simple pattern to extract things in single quotes\n",
    "# as with Approach 1, the for loop prints the results to the screen\n",
    "# you can try and save it to a file if you want to compare with Approach 1 and 3\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern_q = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"},{'ORTH': '\"'}]\n",
    "pattern_q1 = [{'ORTH': '\"'}, {'IS_PUNCT': True, \"OP\": \"+\"},{' ': True, \"OP\": \"+\"},{'ORTH': '\"'}]\n",
    "\n",
    "print(pattern_q)\n",
    "\n",
    "pattern_q = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}]\n",
    "pattern_q1 = [{'ORTH': '“'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '”'}]\n",
    "matcher.add(\"QUOTES\", [pattern_q, pattern_q1], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches_q = matcher(doc)\n",
    "matches_q.sort(key = lambda x: x[1])\n",
    "print (len(matches_q))\n",
    "for match in matches_q[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296dbcc1",
   "metadata": {},
   "source": [
    "## Approach 3: Implemented version\n",
    "This approach was implemented by colleagues at the [Australian Text Analytics Platform](https://www.atap.edu.au/) (ATAP). The approach is based on the [Gender Gap Tracker](https://github.com/sfu-discourse-lab/GenderGapTracker) done in the Discourse Processing Lab here at SFU. \n",
    "\n",
    "The first link below leads you to a binder where you can load your own files and download the output. If you prefer to do everything in your own notebook, you can download/clone the project from GitHub. \n",
    "\n",
    "* [Binder link](https://github.com/Australian-Text-Analytics-Platform/quotation-tool/blob/workshop_01_20220908/README.md)\n",
    "\n",
    "    * Click on the \"binder launch\" button.\n",
    "    * At the CILogin, under \"Select an Identity Provider\", go to the drop-down menu (usually default as ORCID) and select \"Simon Fraser University\".\n",
    "    * This launches [Binder](https://mybinder.readthedocs.io/en/latest/), a service that allows you to run a notebook online on Jupyter Lab (similar to Google Colab). \n",
    "    * Run all the code cells in that notebook, uploading files from the A1_data directory. \n",
    "    * At the end, you can save the output as an Excel file. \n",
    "\n",
    "* [Regular GitHub project](https://github.com/Australian-Text-Analytics-Platform/quotation-tool)\n",
    "\n",
    "    * Run the notebook \"quote_extractor_notebook.ipynb\"\n",
    "\n",
    "Within the ATAP binder, upload 5 files from A1_data (the same you did for approaches 1 and 2), process them and download the results to your own computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8674803d",
   "metadata": {},
   "source": [
    "## Your turn\n",
    "\n",
    "Check instructions on Canvas for what to do and what to submit. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ef8ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
