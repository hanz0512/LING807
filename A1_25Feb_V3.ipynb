{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be347187",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Ling 450/807 SFU - Assignment 1\n",
    "\n",
    "Virginia Uhi, Eunice Wong, & Han Zhang\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cd57cf",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Import packages\n",
    "\n",
    "We import everything we will need here at the beginning and load the spaCy language model. Note that we are using the small English model. One thing you could try is to download and load [other models for English](https://spacy.io/models/en) and compare the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65927718",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing Spacy \n",
    "import spacy\n",
    "import re\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d845f6",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Approach 1: Using regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c311247",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5755aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Some of those ‘soft’ skills are in short supply, but they’re what employers are looking for,', 'There’s a societal cost to having someone remain unemployed,', 'There are health-care costs. There are welfare costs. There can be criminal justice costs,', 'road map', 'should move to skills-based hiring,', 'practices that prioritize … credentials and experience.', 'These are things that are the least susceptible to technological disruption,']\n",
      "[\"It's good that we take the time early on to completely shut hockey off and just relax and then the closer we get to coming back, we've got to get the minds ready,\", \"Most guys were here last year and we learned a lesson from last year when we weren't as good after the break. We know what to do this time and what not to.\", 'He enabled us to at least hang in there and get a point out of this game,', 'The positive is you came in here against one of the best teams and in my opinion, we could have had that game.', \"I don't think we played our best game, but again, we found a way to win and I think that's a big key,\", \"That was one of our weaknesses last year. We couldn't find ways to win and this year, it's the opposite. Good teams, they find ways to win.\", \"Going into the break it's important to go in with a good taste in your mouth,\", \"We played the best when the game was on the line in the third and I thought we had some great looks that made Mrazek make some big saves for them. ... We'll take the two points heading into the break and we'll come back refreshed and ready to go.\"]\n",
      "[\"As a young person, I've experienced that fear and distrust. Just seeing police officers, I feel like I can be also racially targeted,\", \"To avoid any cases like that in the future, it's important to come together and to strategize and to be honest,\", \"What gives me hope is the level of commitment that our police chief has to working alongside FSIN youth because there hasn't been a relationship historically between the FSIN youth representatives and Saskatoon city police,\", 'I thought this was a good time to write a letter to the Saskatoon city police on questioning [if they are] going to be taking body cameras on,', \"And if so, could they kind of explain to FSIN youth what it's gonna look like and what the timeline is.\", 'very committed', 'life-changing and heartbreaking,', \"I can honestly say that it's very emotional to listen to stories of the kind of suffering that goes on in the Indigenous community due to being impoverished, due to violence and due to racism,\", \"The treatment of Indigenous peoples in this country, it's heartbreaking but I can also say that it provides me hope as well when I talk to young people and I see that they persevere.\", \"It may not look perfect to other people, but I can say that I've always been authentically myself I would want all young people to do the same,\", 'I would say there definitely is pressure, but I try to be as gentle and understanding with myself as I would expect to how I should treat another young person.', 'As young people we understand that this land is foundational to who we are,']\n",
      "['I’m so excited and I am so honoured to be selected by you here,', 'My eyes are full of tears because I love this land so deeply,', 'He’s not from our local community. He cannot represent you, represent us,', 'I believe we will have a good chance to win in Burnaby South as I believe right now I’m very familiar with our community. I’m one of the people here.', 'elected', 'Essentially, the leader will choose the candidate in each byelection,', 'whole country']\n",
      "['The Australian Government is concerned about the recent detention of two Canadian citizens in China,', 'We would be very concerned if these cases were related to legal proceedings currently underway in Canada involving a Chinese citizen, Ms. Meng Wanzhou.', 'basically kidnapping', 'every confidence in the fairness and independence of Canada’s administration of justice.', 'We are deeply concerned about the recent detentions of Canadian citizens Michael Kovrig and Michael Spavor in the People’s Republic of China. We ask the Australian Government without further delay to support Canada’s call for the immediate release of these two detainees,', 'deep concern', 'We share Canada’s commitment to the rule of law as fundamental to all free societies, and we will defend and uphold this principle,', 'the declared motive for their investigation raises concerns about legitimate research and business practices in China.', 'The denial of access to a lawyer under their status of detention is contrary to the right of defence. The EU fully supports the efforts of the Canadian Government on this matter.']\n"
     ]
    }
   ],
   "source": [
    "# The starter file loads and processes only one file at a time. We are using the command to do 5 files at the same time\n",
    "\n",
    "# First we take five text files out \n",
    "\n",
    "five_files = [\"A1_data/5c1548a31e67d78e2771624f.txt\", \"A1_data/5c489df91e67d78e271d66c5.txt\", \"A1_data/5c182ac21e67d78e277944ad.txt\", \"A1_data/5c28972a795bd2fac69fa974.txt\", \"A1_data/5c29beda1e67d78e27b74939.txt\"]\n",
    "\n",
    "# Creat a file\n",
    "file_path = \"Regular_Expression_All.txt\"\n",
    "with open(file_path, 'w', encoding='utf-8'):\n",
    "    pass\n",
    "\n",
    "# Process five text files at the same time, and have all the quotes in the five files can be extracted and put in the same file\n",
    "with open (\"Regular_Expression_All.txt\", 'w', encoding = \"utf-8\") as output:\n",
    "    def get_quotes(text):\n",
    "        quotes = re.findall(r'[\"“](.*?)[”\"]', text) \n",
    "            # so that every time as soon as Spacy identify any quotation marks (either straight or curly), the quoted content with the quotation mark will be extracted\n",
    "        return(quotes)\n",
    "    \n",
    "    for file_path in five_files: \n",
    "        output.write(file_path + \"\\n\")\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "                # The starter code seperate the text into sentences at the begining; However, for the sake of better identifying the sentence, we decide to not seperate the sentences first\n",
    "\n",
    "            found_quotes = get_quotes(text) \n",
    "            print(found_quotes)\n",
    "            for quote in found_quotes:\n",
    "                output.write(str(quote) + \"\\n\")  # Using the \"\\n\" to make the quotes listed line by line\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25227db3",
   "metadata": {},
   "source": [
    "## Approach 2: Using spaCy's Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b207ae63",
   "metadata": {},
   "source": [
    "This approach is based on notebooks by [William J.B. Mattingly](https://wjbmattingly.com/). His book, Introduction to Python for Humanists, is available online from the [SFU Library](https://sfu-primo.hosted.exlibrisgroup.com/permalink/f/usv8m3/01SFUL_ALMA51476999620003611). \n",
    "\n",
    "For more on spaCy's Matcher, see Advanced NLP with spaCy, [chapter 2](https://course.spacy.io/en/chapter2)). \n",
    "\n",
    "We have already loaded everything we need at the beginning of this notebook (imported Matcher, assigned it to a `matcher` object), so now we can use it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f0879",
   "metadata": {},
   "source": [
    "## Finding quotes and speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c4213f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert it to a spacy doc\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ee2f7c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Finding proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd311bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is optional. It just tells you who are the people mentioned. You can use it later if you want to find out the speakers of the quotes\n",
    "\n",
    "# matcher = Matcher(nlp.vocab)\n",
    "# pattern_n = [{\"POS\": \"PROPN\"}]\n",
    "# matcher.add(\"PROPER_NOUNS\", [pattern_n], greedy=\"LONGEST\")\n",
    "# doc = nlp(text)\n",
    "# matches = matcher(doc)\n",
    "# print (len(matches))\n",
    "# for match in matches[:10]:\n",
    "    #print (match, doc[match[1]:match[2]])\n",
    "    \n",
    "## You can try to extract full names by adding multi-word nouns, http://spacy.pythonhumanities.com/02_02_matcher.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "debdbaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "(3232560085755078826, 0, 1) Australia\n",
      "(3232560085755078826, 9, 10) Canadians\n",
      "(3232560085755078826, 12, 13) China\n",
      "(3232560085755078826, 30, 31) Canada\n",
      "(3232560085755078826, 39, 44) Foreign Affairs Minister Marise Payne\n",
      "(3232560085755078826, 45, 46) Sunday\n",
      "(3232560085755078826, 55, 56) Australia\n",
      "(3232560085755078826, 71, 72) France\n",
      "(3232560085755078826, 73, 76) New York Times\n",
      "(3232560085755078826, 83, 84) China\n"
     ]
    }
   ],
   "source": [
    "# We skip the he code above and try another method\n",
    "# the starter code for extracting the propoer nouns encounter problem when the propoer noun contains mutiple words\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_n = [{\"POS\": \"PROPN\", \"OP\": \"+\" }] # with OP it will look for propoer noun one or more times\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern_n], greedy=\"LONGEST\")    \n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort (key = lambda x: x[1])  # in case that the results will be organized in descending order based on the length (given the greedy function)\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f92f2",
   "metadata": {},
   "source": [
    "### Finding quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0127acd-6569-42ff-ad1b-7a309b048307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5c488fac1e67d78e271d405b.txt\n",
      "0\n",
      "\n",
      "\n",
      "5c3436991e67d78e27d30c2c.txt\n",
      "6\n",
      "(16432004385153140588, 460, 480) “I walked into the store because I needed cards for school and I met this sassy woman,”\n",
      "(16432004385153140588, 598, 616) “One day I changed the cake recipe and got fired for ‘creative insubordination,’”\n",
      "(16432004385153140588, 761, 790) “I was introduced to someone who worked in the test kitchen of Food And Wine magazine and was asked to write a proposal for a column,”\n",
      "(16432004385153140588, 991, 1006) “They were all chefs and I made them accessible for home cooks,”\n",
      "(16432004385153140588, 1009, 1016) “I loved doing that.”\n",
      "(16432004385153140588, 1077, 1100) “I love doing this work and I never stop thinking how lucky I am to be able to do it,”\n",
      "\n",
      "\n",
      "5c339b091e67d78e27d16414.txt\n",
      "1\n",
      "(16432004385153140588, 634, 647) “use available resources to make educated decisions in choosing food.”\n",
      "\n",
      "\n",
      "5c2afc011e67d78e27ba73a9.txt\n",
      "23\n",
      "(16432004385153140588, 335, 343) “Alexa, open Animal Workout.”\n",
      "(16432004385153140588, 414, 421) “Alexa, start Chompers.”\n",
      "(16432004385153140588, 547, 555) “Alexa, open Sesame Street.”\n",
      "(16432004385153140588, 704, 715) “How does your body feel when you focus?”\n",
      "(16432004385153140588, 731, 741) “Alexa, ask Focus Game to start.”\n",
      "(16432004385153140588, 795, 803) “Alexa, play Kids Quiz!”\n",
      "(16432004385153140588, 867, 875) “Alexa, open Kiwi Monsters!”\n",
      "(16432004385153140588, 952, 960) “Alexa, open Lemonade Stand.”\n",
      "(16432004385153140588, 1062, 1071) “Alexa, open The Magic Door.”\n",
      "(16432004385153140588, 1100, 1107) “Wow in the World,”\n",
      "\n",
      "\n",
      "5c34d7211e67d78e27d54599.txt\n",
      "2\n",
      "(16432004385153140588, 161, 168) \"released in due course.\"\n",
      "(16432004385153140588, 271, 297) \"Kim is eager to remind the Trump administration that he does have diplomatic and economic options besides what Washington and Seoul can offer,\"\n",
      "\n",
      "\n",
      "5c149ffc1e67d78e276fbd44.txt\n",
      "1\n",
      "(16432004385153140588, 514, 530) “This game is all about getting a certain percentage of people to respond,”\n",
      "\n",
      "\n",
      "5c5d532a795bd2d5c282a094.txt\n",
      "6\n",
      "(16432004385153140588, 95, 99) “risk mitigation”\n",
      "(16432004385153140588, 506, 512) “significant upside potential.”\n",
      "(16432004385153140588, 744, 763) “We are obligated to respect the confidentiality of our policy holders and their policies with us,”\n",
      "(16432004385153140588, 948, 952) “risk mitigation”\n",
      "(16432004385153140588, 979, 1007) “EDC had the largest political risk insurance claim charge in its history as a result of the turmoil in North Africa and the Middle East,”\n",
      "(16432004385153140588, 1622, 1643) “but there have been other cases where awards have been paid out for hundreds of millions of dollars.”\n",
      "\n",
      "\n",
      "5c287b841e67d78e27b4163e.txt\n",
      "4\n",
      "(16432004385153140588, 256, 272) \"You can see his impact on our team with the players around him,\"\n",
      "(16432004385153140588, 271, 282) \" DeBoer said of the former Ottawa Senators captain. \"\n",
      "(16432004385153140588, 256, 282) \"You can see his impact on our team with the players around him,\" DeBoer said of the former Ottawa Senators captain. \"\n",
      "(16432004385153140588, 773, 791) \" said the rookie defenceman, younger brother of Seth Jones of the Columbus Blue Jackets. \"\n",
      "\n",
      "\n",
      "5c3e27a31e67d78e27f27b38.txt\n",
      "16\n",
      "(16432004385153140588, 198, 203) “short of breath”\n",
      "(16432004385153140588, 279, 285) “f--ked someone”\n",
      "(16432004385153140588, 290, 298) “plucked the blossom of a woman”\n",
      "(16432004385153140588, 361, 365) “super excited”\n",
      "(16432004385153140588, 504, 511) “from drowning in some bitches”\n",
      "(16432004385153140588, 799, 803) “open up”\n",
      "(16432004385153140588, 1099, 1105) “I … I …”\n",
      "(16432004385153140588, 1298, 1302) “the heck”\n",
      "(16432004385153140588, 1357, 1362) “Never Been Kissed”\n",
      "(16432004385153140588, 1392, 1396) “older girls”\n",
      "\n",
      "\n",
      "5c29947f1e67d78e27b6d330.txt\n",
      "18\n",
      "(16432004385153140588, 88, 96) \"Are you wearing blue shoes?\"\n",
      "(16432004385153140588, 199, 205) \" Bardos blurted out. \"\n",
      "(16432004385153140588, 345, 350) \"the bomb.\"\n",
      "(16432004385153140588, 395, 400) \" she said, \"\n",
      "(16432004385153140588, 399, 407) \"but I am a mutant.\"\n",
      "(16432004385153140588, 395, 407) \" she said, \"but I am a mutant.\"\n",
      "(16432004385153140588, 576, 580) \"litmus test\"\n",
      "(16432004385153140588, 1067, 1073) \"sexual bucket list.\"\n",
      "(16432004385153140588, 1093, 1097) \"cancer drama\"\n",
      "(16432004385153140588, 1132, 1140) \"I see you beyond cancer.\"\n",
      "\n",
      "\n",
      "5c1f1d831e67d78e279c35b4.txt\n",
      "17\n",
      "(16432004385153140588, 220, 225) “very exciting,”\n",
      "(16432004385153140588, 389, 394) “high-normal”\n",
      "(16432004385153140588, 463, 468) “normal-high”\n",
      "(16432004385153140588, 783, 789) “small, healthy choices”\n",
      "(16432004385153140588, 812, 818) “and two belt loops”\n",
      "(16432004385153140588, 830, 849) “I always had good intentions about exercising. But this helped me get with the program,”\n",
      "(16432004385153140588, 906, 917) “My blood pressure is down to normal now,”\n",
      "(16432004385153140588, 941, 960) “I had a health club membership, but this program really helped me develop a habit,”\n",
      "(16432004385153140588, 999, 1007) “was a real eye-opener”\n",
      "(16432004385153140588, 1010, 1017) “No more white bread,”\n",
      "\n",
      "\n",
      "5c33859e1e67d78e27d12893.txt\n",
      "12\n",
      "(16432004385153140588, 622, 636) \"That certainly bears out in our experience here in the morgue.\"\n",
      "(16432004385153140588, 845, 850) \"black box warning\"\n",
      "(16432004385153140588, 1603, 1607) \"nerve pills\"\n",
      "(16432004385153140588, 1635, 1640) \" says LeBlanc. \"\n",
      "(16432004385153140588, 2124, 2128) \"fumbled into\"\n",
      "(16432004385153140588, 2244, 2248) \"acutely aware\"\n",
      "(16432004385153140588, 2247, 2260) \" of what can happen when benzodiazepines are combined with opioids. \"\n",
      "(16432004385153140588, 2244, 2260) \"acutely aware\" of what can happen when benzodiazepines are combined with opioids. \"\n",
      "(16432004385153140588, 2585, 2590) \"another druggie.\"\n",
      "(16432004385153140588, 2746, 2751) \" Golden says. \"\n",
      "\n",
      "\n",
      "5c47fce51e67d78e271b1f7a.txt\n",
      "2\n",
      "(16432004385153140588, 183, 187) “outside eye”\n",
      "(16432004385153140588, 420, 431) “I always felt it deserved to be longer,”\n",
      "\n",
      "\n",
      "5c29ccfc1e67d78e27b76bfb.txt\n",
      "5\n",
      "(16432004385153140588, 179, 194) \"support the rationale for including family members in a veterans treatment plan.\"\n",
      "(16432004385153140588, 348, 373) \"it must be made clear that the full range of benefits and services offers to veterans is NOT offered to family members.\"\n",
      "(16432004385153140588, 601, 629) \"It is part of normal business processes to connect to different areas of the department to ensure that messaging accurately reflects department policy and activity,\"\n",
      "(16432004385153140588, 972, 978) \"large number of emails\"\n",
      "(16432004385153140588, 1007, 1042) \"It was difficult at the outset to understand how someone who suffers from PTSD as a result of a murder they committed should be eligible for health benefits from Veteran Affairs Canada,\"\n",
      "\n",
      "\n",
      "5c1efb3d1e67d78e279bd39a.txt\n",
      "0\n",
      "\n",
      "\n",
      "5c53eea41e67d78e27407cc3.txt\n",
      "1\n",
      "(16432004385153140588, 456, 466) “giving the best care under the circumstances,”\n",
      "\n",
      "\n",
      "5c2ae5f11e67d78e27ba36d7.txt\n",
      "3\n",
      "(16432004385153140588, 311, 320) \"long ways from making a decision,\"\n",
      "(16432004385153140588, 636, 641) \" he said. \"\n",
      "(16432004385153140588, 980, 989) \"the loneliest moment of my life.\"\n",
      "\n",
      "\n",
      "5c1efb3f1e67d78e279bd3b4.txt\n",
      "3\n",
      "(16432004385153140588, 163, 168) \"Accordion Man.\"\n",
      "(16432004385153140588, 543, 555) \"You know, they might throw tomatoes at you!\"\n",
      "(16432004385153140588, 1057, 1064) \"Maybe we started something,\"\n",
      "\n",
      "\n",
      "5c15488f1e67d78e277161d7.txt\n",
      "3\n",
      "(16432004385153140588, 128, 137) “pay up and pollute all you want”\n",
      "(16432004385153140588, 460, 470) “While Conservatives fiddle, the planet burns!”\n",
      "(16432004385153140588, 481, 485) “goal post”\n",
      "\n",
      "\n",
      ".DS_Store\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xf9 in position 347: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(file)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA1_data/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mfile, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f: \u001b[38;5;66;03m#open all files in the folder and read them\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      9\u001b[0m matcher \u001b[38;5;241m=\u001b[39m Matcher(nlp\u001b[38;5;241m.\u001b[39mvocab)\n\u001b[1;32m     11\u001b[0m pattern_q16 \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mORTH\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m}, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIS_ALPHA\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOP\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m\"\u001b[39m}, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIS_PUNCT\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOP\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m}, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIS_ALPHA\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOP\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m\"\u001b[39m}, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIS_PUNCT\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOP\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m}, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mORTH\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m}] \u001b[38;5;66;03m#7\u001b[39;00m\n",
      "File \u001b[0;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xf9 in position 347: invalid start byte"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for file in os.listdir(\"A1_data\"): \n",
    "    print(file)\n",
    "\n",
    "    with open (\"A1_data/\"+file, \"r\", encoding='utf-8') as f: #open all files in the folder and read them\n",
    "        text = f.read()\n",
    "\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    \n",
    "    pattern_q16 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #7\n",
    "    pattern_q17 = [{'ORTH': '“'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '”'}] #4\n",
    "    # both curly and straight quotes\n",
    "    \n",
    "    matcher.add(\"QUOTES\", [pattern_q16, pattern_q17])\n",
    "    doc = nlp(text)\n",
    "    matches_q = matcher(doc)\n",
    "    print(len(matches_q))\n",
    "    for match in matches_q[:10]:\n",
    "        print(match, doc[match[1]:match[2]])\n",
    "    print(\"\\n\") #blank space between outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59f7ad25-c09d-47fe-8f0b-a78dff54447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a text file. Remember, you have to do 5\n",
    "with open (\"A1_data/5c1548a31e67d78e2771624f.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08924137-c0da-4103-a581-aa7e075f5a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/eunicewtk/Desktop/807comptling/LING807Han\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c4070a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "(16432004385153140588, 397, 401) “road map”\n",
      "(16432004385153140588, 413, 423) “should move to skills-based hiring,”\n",
      "(16432004385153140588, 426, 436) “practices that prioritize … credentials and experience.”\n",
      "(16432004385153140588, 628, 642) “These are things that are the least susceptible to technological disruption,”\n"
     ]
    }
   ],
   "source": [
    "# combination of different OP. \n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_q = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"!\"}, {'IS_PUNCT': True, \"OP\": \"!\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q1 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"!\"}, {'IS_PUNCT': True, \"OP\": \"?\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q2 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"!\"}, {'IS_PUNCT': True, \"OP\": \"+\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q3 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"!\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #0\n",
    "\n",
    "pattern_q4 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"?\"}, {'IS_PUNCT': True, \"OP\": \"!\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q5 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"?\"}, {'IS_PUNCT': True, \"OP\": \"?\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q6 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"?\"}, {'IS_PUNCT': True, \"OP\": \"+\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q7 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"?\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #0\n",
    "\n",
    "pattern_q8 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"!\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q9 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"?\"}, {'ORTH': '\"'}] #3\n",
    "pattern_q10 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"+\"}, {'ORTH': '\"'}] #3\n",
    "pattern_q11 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #3\n",
    "\n",
    "pattern_q12 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"!\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q13 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"?\"}, {'ORTH': '\"'}] #3\n",
    "pattern_q14 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"+\"}, {'ORTH': '\"'}] #3\n",
    "pattern_q15 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #3\n",
    "\n",
    "pattern_q16 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #7\n",
    "pattern_q17 = [{'ORTH': '“'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '”'}] #4\n",
    "\n",
    "#pattern_q4 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'ORTH': '\"'}]  #control\n",
    "#pattern_q5 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'ORTH': '.,!?': True, \"OP\": \"+\"}, {'ORTH': '\"'}] \n",
    "#pattern_q6 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'ORTH': '\"'}]\n",
    "#pattern_q7 = [{'ORTH': '\"'}, {'IS_ALPHA': True, 'OP': '?'}, {'ORTH': '\"'}]\n",
    "\n",
    "# Load and extract from txt file 1\n",
    "matcher.add(\"QUOTES\", [pattern_q16, pattern_q17])\n",
    "doc = nlp(text)\n",
    "matches_q = matcher(doc)\n",
    "# matches_q.sort(key = lambda x: x[1])\n",
    "print (len(matches_q))\n",
    "for match in matches_q[:10]:\n",
    "    print (match, doc[match[1]:match[2]])\n",
    "\n",
    "# MAYBE STRAIGHT/CURLY QUOTATION MARKS MATTER, SINCE IT IS RECOGNISING THE MIDDLE PART OF THE QUOTES AS WELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d51814de-cc06-4710-a3a2-c019da63ca58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(16432004385153140588, 331, 351) \"He enabled us to at least hang in there and get a point out of this game,\"\n",
      "(16432004385153140588, 358, 385) \"The positive is you came in here against one of the best teams and in my opinion, we could have had that game.\"\n",
      "(16432004385153140588, 706, 711) \" Peters said. \"\n"
     ]
    }
   ],
   "source": [
    "# Load and extract from txt file 2\n",
    "with open (\"A1_data/5c489df91e67d78e271d66c5.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "pattern_q16 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #7\n",
    "pattern_q17 = [{'ORTH': '“'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '”'}] #4\n",
    "\n",
    "matcher.add(\"QUOTES\", [pattern_q16, pattern_q17])\n",
    "doc = nlp(text)\n",
    "matches_q = matcher(doc)\n",
    "# matches_q.sort(key = lambda x: x[1])\n",
    "print (len(matches_q))\n",
    "for match in matches_q[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "977cc086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "(16432004385153140588, 366, 370) \"very committed\"\n",
      "(16432004385153140588, 445, 453) \"life-changing and heartbreaking,\"\n",
      "(16432004385153140588, 596, 630) \"I would say there definitely is pressure, but I try to be as gentle and understanding with myself as I would expect to how I should treat another young person.\"\n",
      "(16432004385153140588, 677, 694) \"As young people we understand that this land is foundational to who we are,\"\n"
     ]
    }
   ],
   "source": [
    "# Load and extract from txt file 3\n",
    "with open (\"A1_data/5c182ac21e67d78e277944ad.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "pattern_q16 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #7\n",
    "pattern_q17 = [{'ORTH': '“'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '”'}] #4\n",
    "\n",
    "matcher.add(\"QUOTES\", [pattern_q16, pattern_q17])\n",
    "doc = nlp(text)\n",
    "matches_q = matcher(doc)\n",
    "# matches_q.sort(key = lambda x: x[1])\n",
    "print (len(matches_q))\n",
    "for match in matches_q[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ec8937d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(16432004385153140588, 128, 144) “My eyes are full of tears because I love this land so deeply,”\n",
      "(16432004385153140588, 576, 590) “Essentially, the leader will choose the candidate in each byelection,”\n",
      "(16432004385153140588, 619, 623) “whole country”\n"
     ]
    }
   ],
   "source": [
    "# Load and extract from txt file 4\n",
    "with open (\"A1_data/5c28972a795bd2fac69fa974.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "pattern_q16 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #7\n",
    "pattern_q17 = [{'ORTH': '“'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '”'}] #4\n",
    "\n",
    "matcher.add(\"QUOTES\", [pattern_q16, pattern_q17])\n",
    "doc = nlp(text)\n",
    "matches_q = matcher(doc)\n",
    "# matches_q.sort(key = lambda x: x[1])\n",
    "print (len(matches_q))\n",
    "for match in matches_q[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "735e772d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(16432004385153140588, 128, 146) “The Australian Government is concerned about the recent detention of two Canadian citizens in China,”\n",
      "(16432004385153140588, 265, 269) “basically kidnapping”\n",
      "(16432004385153140588, 514, 518) “deep concern”\n",
      "(16432004385153140588, 566, 585) “the declared motive for their investigation raises concerns about legitimate research and business practices in China.”\n",
      "(16432004385153140588, 587, 623) “The denial of access to a lawyer under their status of detention is contrary to the right of defence. The EU fully supports the efforts of the Canadian Government on this matter.”\n"
     ]
    }
   ],
   "source": [
    "# Load and extract from txt file 5\n",
    "with open (\"A1_data/5c29beda1e67d78e27b74939.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "pattern_q16 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #7\n",
    "pattern_q17 = [{'ORTH': '“'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '”'}] #4\n",
    "\n",
    "matcher.add(\"QUOTES\", [pattern_q16, pattern_q17])\n",
    "doc = nlp(text)\n",
    "matches_q = matcher(doc)\n",
    "# matches_q.sort(key = lambda x: x[1])\n",
    "print (len(matches_q))\n",
    "for match in matches_q[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296dbcc1",
   "metadata": {},
   "source": [
    "## Approach 3: Implemented version\n",
    "This approach was implemented by colleagues at the [Australian Text Analytics Platform](https://www.atap.edu.au/) (ATAP). The approach is based on the [Gender Gap Tracker](https://github.com/sfu-discourse-lab/GenderGapTracker) done in the Discourse Processing Lab here at SFU. \n",
    "\n",
    "The first link below leads you to a binder where you can load your own files and download the output. If you prefer to do everything in your own notebook, you can download/clone the project from GitHub. \n",
    "\n",
    "* [Binder link](https://github.com/Australian-Text-Analytics-Platform/quotation-tool/blob/workshop_01_20220908/README.md)\n",
    "\n",
    "    * Click on the \"binder launch\" button.\n",
    "    * At the CILogin, under \"Select an Identity Provider\", go to the drop-down menu (usually default as ORCID) and select \"Simon Fraser University\".\n",
    "    * This launches [Binder](https://mybinder.readthedocs.io/en/latest/), a service that allows you to run a notebook online on Jupyter Lab (similar to Google Colab). \n",
    "    * Run all the code cells in that notebook, uploading files from the A1_data directory. \n",
    "    * At the end, you can save the output as an Excel file. \n",
    "\n",
    "* [Regular GitHub project](https://github.com/Australian-Text-Analytics-Platform/quotation-tool)\n",
    "\n",
    "    * Run the notebook \"quote_extractor_notebook.ipynb\"\n",
    "\n",
    "Within the ATAP binder, upload 5 files from A1_data (the same you did for approaches 1 and 2), process them and download the results to your own computer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f6ad8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8674803d",
   "metadata": {},
   "source": [
    "## Your turn\n",
    "\n",
    "Check instructions on Canvas for what to do and what to submit. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ef8ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
