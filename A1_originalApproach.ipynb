{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be347187",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Ling 450/807 SFU - Assignment 1\n",
    "\n",
    "Virginia Uhi, Eunice Wong, & Han Zhang\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cd57cf",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Import packages\n",
    "\n",
    "We import everything we will need here at the beginning and load the spaCy language model. Note that we are using the small English model. One thing you could try is to download and load [other models for English](https://spacy.io/models/en) and compare the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65927718",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing Spacy \n",
    "import os\n",
    "import spacy\n",
    "import re\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d845f6",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Approach 1: Using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "abac4706-1e85-485b-970f-fc42ba202218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".ipynb_checkpoints\n",
      "5c2059ec1e67d78e279ea86c.txt\n",
      "5c201b371e67d78e279e248a.txt\n",
      "5c20ae45795bd2d89328853e.txt\n",
      "5c2060d31e67d78e279eb852.txt\n",
      "5c1f328f1e67d78e279c7d31.txt\n"
     ]
    }
   ],
   "source": [
    "five_files = [\"A1_data/5c1548a31e67d78e2771624f.txt\", \"A1_data/5c489df91e67d78e271d66c5.txt\", \"A1_data/5c182ac21e67d78e277944ad.txt\", \"A1_data/5c28972a795bd2fac69fa974.txt\", \"A1_data/5c29beda1e67d78e27b74939.txt\"]\n",
    "folder_path = [\"LING807 Compuling/A1_grp3_txt\"]\n",
    "\n",
    "for file in os.listdir(\"A1_grp3_txt\"): \n",
    "    print(file)\n",
    "\n",
    "#    with open (folder_path, 'w',encoding=\"utf-8\") as output:\n",
    "\n",
    "# Process five text files at the same time, and have all the quotes in the five files can be extracted and put in the same file\n",
    "#with open (five_files, 'w', encoding = \"utf-8\") as output:\n",
    "    def get_quotes(text):\n",
    "            quotes = re.findall(r'[\"“](.*?)[”\"]', text) \n",
    "            # so that every time as soon as Spacy identify any quotation marks (either straight or curly), the quoted content with the quotation mark will be extracted\n",
    "            return(quotes)\n",
    "    \n",
    "#    for file_path in five_files: \n",
    " #       output.write(file_path + \"\\n\")\n",
    "  #      with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "   #         text = file.read()\n",
    "                # The starter code seperate the text into sentences at the begining; However, for the sake of better identifying the sentence, we decide to not seperate the sentences first\n",
    "\n",
    "            found_quotes = get_quotes(text) \n",
    "            print(found_quotes)\n",
    "            for quote in found_quotes:\n",
    "                output.write(str(quote) + \"\\n\")  # Using the \"\\n\" to make the quotes listed line by line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c2aea16-5b94-4392-a32b-d2ec31f72361",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m five_files \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA1_data/5c1548a31e67d78e2771624f.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA1_data/5c489df91e67d78e271d66c5.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA1_data/5c182ac21e67d78e277944ad.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA1_data/5c28972a795bd2fac69fa974.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA1_data/5c29beda1e67d78e27b74939.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Process five text files at the same time, and have all the quotes in the five files can be extracted and put in the same file\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mfive_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m output:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_quotes\u001b[39m(text):\n\u001b[1;32m     10\u001b[0m         quotes \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m“](.*?)[”\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m, text) \n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not list"
     ]
    }
   ],
   "source": [
    "# The starter file loads and processes only one file at a time. We are using the command to do 5 files at the same time\n",
    "\n",
    "# First we take five text files out \n",
    "\n",
    "five_files = [\"A1_data/5c1548a31e67d78e2771624f.txt\", \"A1_data/5c489df91e67d78e271d66c5.txt\", \"A1_data/5c182ac21e67d78e277944ad.txt\", \"A1_data/5c28972a795bd2fac69fa974.txt\", \"A1_data/5c29beda1e67d78e27b74939.txt\"]\n",
    "\n",
    "# Process five text files at the same time, and have all the quotes in the five files can be extracted and put in the same file\n",
    "with open (five_files, 'w', encoding = \"utf-8\") as output:\n",
    "    def get_quotes(text):\n",
    "        quotes = re.findall(r'[\"“](.*?)[”\"]', text) \n",
    "            # so that every time as soon as Spacy identify any quotation marks (either straight or curly), the quoted content with the quotation mark will be extracted\n",
    "        return(quotes)\n",
    "    \n",
    "    for file_path in five_files: \n",
    "        output.write(file_path + \"\\n\")\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "                # The starter code seperate the text into sentences at the begining; However, for the sake of better identifying the sentence, we decide to not seperate the sentences first\n",
    "\n",
    "            found_quotes = get_quotes(text) \n",
    "            print(found_quotes)\n",
    "            for quote in found_quotes:\n",
    "                output.write(str(quote) + \"\\n\")  # Using the \"\\n\" to make the quotes listed line by line\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25227db3",
   "metadata": {},
   "source": [
    "## Approach 2: Using spaCy's Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b207ae63",
   "metadata": {},
   "source": [
    "This approach is based on notebooks by [William J.B. Mattingly](https://wjbmattingly.com/). His book, Introduction to Python for Humanists, is available online from the [SFU Library](https://sfu-primo.hosted.exlibrisgroup.com/permalink/f/usv8m3/01SFUL_ALMA51476999620003611). \n",
    "\n",
    "For more on spaCy's Matcher, see Advanced NLP with spaCy, [chapter 2](https://course.spacy.io/en/chapter2)). \n",
    "\n",
    "We have already loaded everything we need at the beginning of this notebook (imported Matcher, assigned it to a `matcher` object), so now we can use it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f0879",
   "metadata": {},
   "source": [
    "## Finding quotes and speakers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ee2f7c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Finding proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd311bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "(3232560085755078826, 1, 2) CTV\n",
      "(3232560085755078826, 2, 3) Vancouver\n",
      "(3232560085755078826, 6, 7) Abbotsford\n",
      "(3232560085755078826, 8, 9) B.C.\n",
      "(3232560085755078826, 25, 26) Africa\n",
      "(3232560085755078826, 42, 43) Kim\n",
      "(3232560085755078826, 44, 45) Clark\n",
      "(3232560085755078826, 45, 46) Moran\n",
      "(3232560085755078826, 52, 53) Immigration\n",
      "(3232560085755078826, 54, 55) Refugees\n"
     ]
    }
   ],
   "source": [
    "# This is optional. It just tells you who are the people mentioned. You can use it later if you want to find out the speakers of the quotes\n",
    "\n",
    "# matcher = Matcher(nlp.vocab)\n",
    "# pattern_n = [{\"POS\": \"PROPN\"}]\n",
    "# matcher.add(\"PROPER_NOUNS\", [pattern_n], greedy=\"LONGEST\")\n",
    "# doc = nlp(text)\n",
    "# matches = matcher(doc)\n",
    "# print (len(matches))\n",
    "# for match in matches[:10]:\n",
    "    #print (match, doc[match[1]:match[2]])\n",
    "    \n",
    "## You can try to extract full names by adding multi-word nouns, http://spacy.pythonhumanities.com/02_02_matcher.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "debdbaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "(3232560085755078826, 0, 1) Australia\n",
      "(3232560085755078826, 9, 10) Canadians\n",
      "(3232560085755078826, 12, 13) China\n",
      "(3232560085755078826, 30, 31) Canada\n",
      "(3232560085755078826, 39, 44) Foreign Affairs Minister Marise Payne\n",
      "(3232560085755078826, 45, 46) Sunday\n",
      "(3232560085755078826, 55, 56) Australia\n",
      "(3232560085755078826, 71, 72) France\n",
      "(3232560085755078826, 73, 76) New York Times\n",
      "(3232560085755078826, 83, 84) China\n"
     ]
    }
   ],
   "source": [
    "# We escape the he code above and try another method\n",
    "# the starter code for extracting the propoer nouns encounter problem when the propoer noun contains mutiple words\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_n = [{\"POS\": \"PROPN\", \"OP\": \"+\" }] # with OP it will look for propoer noun one or more times\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern_n], greedy=\"LONGEST\")    \n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort (key = lambda x: x[1])  # in case that the results will be organized in descending order based on the length (given the greedy function)\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f92f2",
   "metadata": {},
   "source": [
    "### Finding quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4e50a4a-1e2b-4eda-90b5-7a4f03203b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".ipynb_checkpoints\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: 'A1_grp3_txt/.ipynb_checkpoints'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA1_grp3_txt\u001b[39m\u001b[38;5;124m\"\u001b[39m): \n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(file)\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mA1_grp3_txt/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f: \u001b[38;5;66;03m#open all files in the folder and read them\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      7\u001b[0m     matcher \u001b[38;5;241m=\u001b[39m Matcher(nlp\u001b[38;5;241m.\u001b[39mvocab)\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: 'A1_grp3_txt/.ipynb_checkpoints'"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(\"A1_grp3_txt\"): \n",
    "    print(file)\n",
    "\n",
    "    with open (\"A1_grp3_txt/\"+file, \"r\", encoding='utf-8') as f: #open all files in the folder and read them\n",
    "        text = f.read()\n",
    "    \n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    \n",
    "    pattern_q16 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #7\n",
    "    pattern_q17 = [{'ORTH': '“'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '”'}] #4\n",
    "    # both curly and straight quotes\n",
    "    \n",
    "    matcher.add(\"QUOTES\", [pattern_q16, pattern_q17])\n",
    "    doc = nlp(text)\n",
    "    matches_q = matcher(doc)\n",
    "    print(len(matches_q))\n",
    "    for match in matches_q[:10]:\n",
    "        print(match, doc[match[1]:match[2]])\n",
    "    print(\"\\n\") #blank space between outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08924137-c0da-4103-a581-aa7e075f5a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/a1a01287-00fd-4f66-9d41-05fe0b880957/LING807 Compuling\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c4070a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5c33859e1e67d78e27d12893.txt\n",
      "5c5404491e67d78e274131a2.txt\n",
      "5c4962f31e67d78e271f9498.txt\n",
      "5c32f9841e67d78e27cfa4eb.txt\n",
      "5c3370aa1e67d78e27d0f869.txt\n",
      "5c32e5141e67d78e27cf6aeb.txt\n",
      "5c5da7aa1e67d78e275f8a3c.txt\n",
      "5c5d3e251e67d78e275e54b5.txt\n",
      "5c146e42795bd2fcce2ea8e5.txt\n",
      "5c29ccfc1e67d78e27b76bfb.txt\n",
      "5c182ac21e67d78e277944ad.txt\n",
      "5c533fe21e67d78e273e92d1.txt\n",
      "5c483b26795bd2b724e92a68.txt\n",
      "5c50a9ee1e67d78e27364c05.txt\n",
      "5c49ef691e67d78e272197a5.txt\n",
      "5c2aa6971e67d78e27b9ab24.txt\n",
      "5c54662d1e67d78e27425afa.txt\n",
      "5c498cc6795bd264151080e0.txt\n",
      "5c3f55241e67d78e27f63e5a.txt\n",
      "5c53fcaf1e67d78e2740a25d.txt\n",
      "5c49e1261e67d78e2721712b.txt\n",
      "5c541912795bd22bf37ca662.txt\n",
      "5c1e0b68795bd2a5d03a49a9.txt\n",
      "5c2858471e67d78e27b3b633.txt\n",
      "5c1f08711e67d78e279bf66d.txt\n",
      ".ipynb_checkpoints\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: 'A1_data/.ipynb_checkpoints'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA1_data\u001b[39m\u001b[38;5;124m\"\u001b[39m): \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(file)\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mA1_data/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f: \u001b[38;5;66;03m#open all files in the folder and read them\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      8\u001b[0m matcher \u001b[38;5;241m=\u001b[39m Matcher(nlp\u001b[38;5;241m.\u001b[39mvocab)\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: 'A1_data/.ipynb_checkpoints'"
     ]
    }
   ],
   "source": [
    "# combination of different OP. \n",
    "for file in os.listdir(\"A1_data\"): \n",
    "    print(file)\n",
    "\n",
    "    with open (\"A1_data/\"+file, \"r\", encoding='utf-8') as f: #open all files in the folder and read them\n",
    "        text = f.read()\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_q = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"!\"}, {'IS_PUNCT': True, \"OP\": \"!\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q1 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"!\"}, {'IS_PUNCT': True, \"OP\": \"?\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q2 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"!\"}, {'IS_PUNCT': True, \"OP\": \"+\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q3 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"!\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #0\n",
    "\n",
    "pattern_q4 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"?\"}, {'IS_PUNCT': True, \"OP\": \"!\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q5 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"?\"}, {'IS_PUNCT': True, \"OP\": \"?\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q6 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"?\"}, {'IS_PUNCT': True, \"OP\": \"+\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q7 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"?\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #0\n",
    "\n",
    "pattern_q8 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"!\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q9 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"?\"}, {'ORTH': '\"'}] #3\n",
    "pattern_q10 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"+\"}, {'ORTH': '\"'}] #3\n",
    "pattern_q11 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #3\n",
    "\n",
    "pattern_q12 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"!\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q13 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"?\"}, {'ORTH': '\"'}] #3\n",
    "pattern_q14 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"+\"}, {'ORTH': '\"'}] #3\n",
    "pattern_q15 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #3\n",
    "\n",
    "pattern_q16 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #7\n",
    "pattern_q17 = [{'ORTH': '“'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '”'}] #4\n",
    "pattern_q18 = [{'LOWER':{'IN':['said', 'say','says']}},{'LOWER': 'that'}] #{'IS_ALPHA': True, 'OP': '*'},{'IS_PUNCT': True, \"OP\": \"*\"},{'IS_ALPHA': True, 'OP': '*'}]\n",
    "\n",
    "#pattern_q4 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'ORTH': '\"'}]  #control\n",
    "#pattern_q5 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'ORTH': '.,!?': True, \"OP\": \"+\"}, {'ORTH': '\"'}] \n",
    "#pattern_q6 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'ORTH': '\"'}]\n",
    "#pattern_q7 = [{'ORTH': '\"'}, {'IS_ALPHA': True, 'OP': '?'}, {'ORTH': '\"'}]\n",
    "\n",
    "matcher.add(\"QUOTES\", [pattern_q4])\n",
    "doc = nlp(text)\n",
    "matches_q = matcher(doc)\n",
    "# matches_q.sort(key = lambda x: x[1])\n",
    "print (len(matches_q))\n",
    "for match in matches_q[:10]:\n",
    "    print (match, doc[match[1]:match[2]])\n",
    "\n",
    "# MAYBE STRAIGHT/CURLY QUOTATION MARKS MATTER, SINCE IT IS RECOGNISING THE MIDDLE PART OF THE QUOTES AS WELL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296dbcc1",
   "metadata": {},
   "source": [
    "## Approach 3: Implemented version\n",
    "This approach was implemented by colleagues at the [Australian Text Analytics Platform](https://www.atap.edu.au/) (ATAP). The approach is based on the [Gender Gap Tracker](https://github.com/sfu-discourse-lab/GenderGapTracker) done in the Discourse Processing Lab here at SFU. \n",
    "\n",
    "The first link below leads you to a binder where you can load your own files and download the output. If you prefer to do everything in your own notebook, you can download/clone the project from GitHub. \n",
    "\n",
    "* [Binder link](https://github.com/Australian-Text-Analytics-Platform/quotation-tool/blob/workshop_01_20220908/README.md)\n",
    "\n",
    "    * Click on the \"binder launch\" button.\n",
    "    * At the CILogin, under \"Select an Identity Provider\", go to the drop-down menu (usually default as ORCID) and select \"Simon Fraser University\".\n",
    "    * This launches [Binder](https://mybinder.readthedocs.io/en/latest/), a service that allows you to run a notebook online on Jupyter Lab (similar to Google Colab). \n",
    "    * Run all the code cells in that notebook, uploading files from the A1_data directory. \n",
    "    * At the end, you can save the output as an Excel file. \n",
    "\n",
    "* [Regular GitHub project](https://github.com/Australian-Text-Analytics-Platform/quotation-tool)\n",
    "\n",
    "    * Run the notebook \"quote_extractor_notebook.ipynb\"\n",
    "\n",
    "Within the ATAP binder, upload 5 files from A1_data (the same you did for approaches 1 and 2), process them and download the results to your own computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8674803d",
   "metadata": {},
   "source": [
    "## New approach\n",
    "\n",
    "Check instructions on Canvas for what to do and what to submit. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ef8ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(\"A1_data\"): \n",
    "    print(file)\n",
    "\n",
    "    with open (\"A1_data\"+file, \"r\", encoding='utf-8') as f: #open all files in the folder and read them\n",
    "        text = f.read()\n",
    "    \n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    \n",
    "    pattern_1 = [{}] #7\n",
    "    pattern_q17 = [{'ORTH': '“'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '”'}] #4\n",
    "    # both curly and straight quotes\n",
    "    \n",
    "    matcher.add(\"QUOTES\", [pattern_q16, pattern_q17])\n",
    "    doc = nlp(text)\n",
    "    matches_q = matcher(doc)\n",
    "    print(len(matches_q))\n",
    "    for match in matches_q[:10]:\n",
    "        print(match, doc[match[1]:match[2]])\n",
    "    print(\"\\n\") #blank space between outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f584771f-4041-4994-b1a8-8fd3a371f898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reported speech found: said that he\n",
      "Reported speech found: said that he would\n",
      "Reported speech found: said that he would come\n",
      "Reported speech found: said that he would come tomorrow\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define the pattern to match reported speech or indirect quotes\n",
    "pattern_reported_speech = [\n",
    "    {'LOWER': {'IN': ['said', 'say']}},\n",
    "    {'LOWER': 'that'},\n",
    "    {'POS': {'IN': ['ADP', 'NOUN', 'PRON', 'PROPN']}, 'OP': '*'},  # Optionally match any preposition, noun, pronoun, or proper noun\n",
    "    {'IS_ALPHA': True, 'OP': '+'}  # Match one or more alphabetical characters (the reported speech)\n",
    "]\n",
    "\n",
    "matcher.add(\"REPORTED_SPEECH\", [pattern_reported_speech])\n",
    "\n",
    "# Example text containing reported speech\n",
    "text = \"He said that he would come tomorrow. She says that she likes ice cream. They said, 'We are going to the beach.'\"\n",
    "\n",
    "doc = nlp(text)\n",
    "matches_reported_speech = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches_reported_speech:\n",
    "    reported_speech = doc[start:end].text\n",
    "    print(\"Reported speech found:\", reported_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b1b8efd8-8d29-4cc5-957c-eda5f3d53fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question was common for mayoral candidates to hear back in the October municipal election: what do you think of ride-hailing and should it come to Metro Vancouver?. \n",
      " “I was clear when I was mayor – I don’t support Uber at all,” then-Surrey mayoral candidate Doug McCallum responded at one debate. \n",
      " It was an odd thing to say: McCallum’s last term as Surrey mayor ended in 2005, while Uber as a company began in 2009, briefly entering the Vancouver market only in 2012. There was no Uber not to support in 2005. \n",
      " Rival candidate Bruce Hayne joked in his turn to answer the question, “It was a twinkle in some engineer’s eye some years ago.”. \n",
      " McCallum didn’t correct himself. \n",
      " But the strange remark may have foreshadowed a growing number of curious statements from the returning mayor of B.C.’s second-largest, rapidly growing city, which is wrestling with big changes after his come-from-behind election win. \n",
      " CTV News has analyzed about three weeks' worth of McCallum's speeches, statements at public meetings, and press conferences to find the mayor’s claims, in some cases, don’t stand up to inspection. \n",
      " Sometimes, his claims seem to further his political agenda, such as exaggerating a drop in the children participating in ice sports, which was part of his argument to cut the previous administration's projects, including a $44.5 million Cloverdale Sport and Ice Complex. \n",
      " A claim the city's debt is higher than city reports say it is could support a total of $135 million in proposed cuts to the previous administration's projects, including the $58 million Grandview Heights Community Centre and Library. \n",
      " A claim the costs of building a SkyTrain extension to Langley are lower than TransLink says they are could make one of McCallum's two big-ticket promises, SkyTrain and a municipal police force, more attractive. \n",
      " But other times, the claims, such as what he said about Uber, do not seem to advance any particular political agenda. To make our list, the claims had to be on camera, at a public meeting or addressed to the press, and not be corrected as a one-off error. \n",
      " Some voters say he is already losing their trust. \n",
      " “Mayor McCallum’s statements vary greatly from truth,” Cindy Dalglish told Surrey’s councillors at Tuesday’s finance committee meeting. \n",
      " But observers say there may be a method to McCallum’s messages. \n",
      " “There’s a tried-and-true method in Canadian politics: after an election a new government takes office and says, ‘Oh my gosh, the cupboards are bare.’ Or, ‘We’re much deeper in debt than I thought we were, and now I’ve seen the real books.' So I think there’s an element of that kind of gamesmanship going on,” University of the Fraser Valley political scientist Hamish Telford said. \n",
      " “Then there’s the fact that McCallum has been out of office for quite some time, thinking he knew the job, but some things have changed,” Telford said. \n",
      " McCallum didn’t respond to interview requests on these topics through City of Surrey staff, so CTV News caught up with him at Thursday’s Mayors' Council meeting. \n",
      " The mayor didn’t respond directly to questions about whether there’s a pattern to advancing these claims, but CTV News has included the response he did give in each section below. \n",
      " . \n",
      " Citing a nonexistent TransLink study. \n",
      " “If you take Fraser Highway SkyTrain and if we’re building that seven days a week around the clock, we probably can save, and this is TransLink’s figures, we can probably save $2-300 million,” McCallum told the Mayors Council on Nov. 15. \n",
      " McCallum’s speech to fellow municipal leaders was a crucial part of getting the region on board with his plan to switch from the Newton-Surrey-Guildford LRT to a SkyTrain extension to Langley. \n",
      " Mayors were understandably concerned about the cost of the move to a Langley SkyTrain, which a TransLink study in 2017 concluded would cost between $2.6 billion and $2.9 billion – almost twice the $1.65 billion cost of the LRT line. \n",
      " McCallum claimed in that speech the cost could be reduced if the construction was done 24/7, which may be possible, though it could lead to substantial disruptions along the route for the public. \n",
      " But the mayor claimed a study had been done to confirm this by TransLink. The agency told CTV News it is has not done any such study. \n",
      " “TransLink has not conducted any detailed study on potential construction methods for a SkyTrain route from Surrey to Langley. The most recent cost estimate (2017 Hatch report) did not consider construction schedules,” a spokesperson said. \n",
      " On Dec. 13, the mayor brought up the 24/7 construction plan up again while speaking to the media. \n",
      " “We’ve also said in the past if we let large construction jobs build seven days a week, around the clock, we can reduce costs considerably,” he said. When told of TransLink’s statement they had never done a study, he said, “I’ve never said there’s a study.”. \n",
      " Tap the image above to read more about the cost of a SkyTrain line, and who could be on the hook for the switch from LRT. (Photo: City of Surrey). \n",
      " . \n",
      " SkyTrain vs. Evergreen Line: inaccurate comparisons. \n",
      " “The Evergreen Line that was recently built, same distance, elevated all the way, actually goes through a tunnel, completed a year and a half ago, for only $1.4 billion,” McCallum told reporters on Oct. 23. \n",
      " It’s true the Evergreen Line was built for $1.41 billion, opening in 2016. Its length is 10.9 kilometres, and does include a tunnel. But the Evergreen Line is about two-thirds the length of a line that would run from King George Station along Fraser Highway to Langley City. The SkyTrain extension would be about 16 kilometres long. \n",
      " McCallum has said the Fraser Highway extension could be built affordably for the funding envelope that had already been approved for the LRT, at $1.65 billion. That's a claim B.C.'s premier didn't buy. \n",
      " “I care not about the technology. If Surrey wants to change to SkyTrain, that’s fine, but they’re going to get half as far as they would have with LRT,” John Horgan told reporters on Thursday. \n",
      " TransLink staff are currently doing design and development work on the SkyTrain extension. TransLink CEO Kevin Desmond said Thursday one of McCallum's suggestions, to find places where the line could not be elevated, could save money. \n",
      " While the agency doesn’t have a final cost estimate for the SkyTrain line, the Mayors' Council has heard it can’t be built for the $1.6 billion left after $56 million was spent on pre-construction and design for the LRT. \n",
      " The Mayors' Council heard Thursday that TransLink was at least $1 billion short for the cost of the full line, and it may have to terminate in Fleetwood or Clayton Heights. \n",
      " But coming up with lower lengths, and therefore lower costs, could be part of the mayor’s attempts to make the SkyTrain extension seem more attractive, said Telford. \n",
      " “I think he’s trying to present the best possible case that he can fulfill his promises without too much financial pain, and to put it generously, he’s reading the spreadsheets optimistically,” Telford said. \n",
      " Click the image above to view details of the plan discused at the Dec. 13 Mayors' Council meeting. \n",
      " . \n",
      " Overstating Surrey's debt. \n",
      " A press release, issued just after midnight on Nov. 27, McCallum expressed serious concern about the Surrey’s debt. \n",
      " “When the books were shown to me, I was deeply dismayed and shaken to the core to see how much debt the City of Surrey has been carrying,” he was quoted as saying in the press release. “The fact that the debt load is at $514 million is simply untenable and frankly irresponsible.”. \n",
      " City financial records from December 2017 indicated the debt load was then $267 million, or just over half that amount. And in the city’s proposed budget, issued under McCallum’s council for a Dec. 11, 2018 meeting, the “previously approved” internal and external debt is $316 million – only about 60 per cent of the mayor’s claim. \n",
      " McCallum’s staff later said the $514 million figure he was referring to was the debt load the city is expected to take on over five years of building projects, which by definition is money that hasn't been spent yet. \n",
      " In the proposed budget, the $514 million figure includes “previously approved” debt and $198 million of “internal debt approved for the 2018-2022 General Capital Program.”. \n",
      " When asked on Dec. 13 whether he was overstating the city’s debt, McCallum said, “The city’s debt as far as the City of Surrey is $514 million, that’s in the financial statements, in the public documents that have gone out. I’ve never misstated that debt. So that’s there. We’ve passed a draft budget. It takes into consideration the very large debt that we need to bring down.”. \n",
      " McCallum’s council narrowly approved a plan to slash $136 million that hasn’t been spent yet on proposed capital projects, including upgrades to RCMP facilities, a part, the Grandview Heights Library and Community Centre and a new sport and ice complex in Cloverdale. \n",
      " . \n",
      " Overstating the number of Surrey's rinks. \n",
      " One of the projects McCallum's Safe Surrey council plans to pull the plug on is the $44.5 million Cloverdale Sport and Ice Complex. In a media availability shortly after the proposed budget was made public, McCallum was upbeat about the rinks the city still had for its citizens. \n",
      " “We have one ice rink in South Surrey, we have one in Cloverdale, we have three in Fleetwood which is only 10 minutes from Cloverdale. We have one in Newton, and we have two in our city centre. We have three opening brand new in the middle of next year in Bridgeview, by the Patullo Bridge. We have the three new ones coming along with the current ones,” he said. \n",
      " The new Bridgeview complex will indeed have three new rinks. But the project is slated to replace the North Surrey Arena, which has two rinks, according to the city, and that rink is slated to close shortly after the Bridgeview complex opens. It may be possible to keep both rinks open, but city staff confirmed there are no plans to do this at this point. \n",
      " “We found that a little bit misleading, they were talking about all the new arenas, but in reality we won’t be gaining three. Two of the arenas will be taken away. You’re only getting one new rink,” Cloverdale Minor Hockey Association president Marty Jones told CTV News. \n",
      " Tap the image above to read the city's plans for the Cloverdale Sport and Ice Complex, initially expected to open in 2020. \n",
      " . \n",
      " Claims about the Cloverdale complex costs. \n",
      " McCallum also claimed of the Cloverdale Sport and Ice Complex, “The land underneath is very unstable. Our engineering department says we would not be able to build this on unstable ground unless we used pile driving techniques to build it. Pile driving if anyone knows construction is very expensive construction. Originally it was planned to be $45 million, but that would increase considerably if we had to pile drive it.”. \n",
      " In fact, the original cost of the Cloverdale complex was $35 million, city documents say. In January 2018, reports indicated there were problems with the soil and the estimated cost was hiked to $44.5 million. \n",
      " Attempts by CTV News to confirm with the city’s engineering department that there were even more problems with the soil on top of the previously discovered issues, which could increase costs even further, were stymied as city staff said they could not talk during the budget process. \n",
      " In the city’s preliminary budget in December, the cost of the arena is still listed at $44.5 million. \n",
      " The above chart from the City of Surrey suggests a dip in registration for youth ice sports. \n",
      " . \n",
      " Exaggerating a drop in youth ice sports. \n",
      " The mayor also claimed that the enrolment of young people in ice sports was dropping – another justification for cutting the construction of the Cloverdale complex. \n",
      " “We have in the last eight to nine years had a declining use of ice for hockey. Each year it’s declining in participation and usage. We have a chart of that to show you,” the mayor told reporters. \n",
      " CTV News has analyzed the chart, which seemed to show a decline in enrolment of around 50 per cent. But the axes of the chart exaggerated this drop. When the axes were changed, the decline the numbers represented appeared closer to 15 per cent. \n",
      " There was another problem. \n",
      " For every year before 2018-19, the chart reflected the number of children playing ice sports for the whole year. But for the 2018-19 year, staff used the figures for October. The result, according to a survey of listed clubs by CTV News, is 352 missing children. \n",
      " When those members are included in the chart, it’s clear that there are more children enrolled in 2018-19 than the past two years – and the alleged drop is actually an increase. \n",
      " On top of that, the chart did not include bantam or midget teams – another 100 children – anyone in the city’s ice lessons, which includes 17,040 registrations, or anyone on the waitlist, whose estimates run from the city claim of 974 to the city’s Ice Allocation Committee estimate of 1,400. \n",
      " “I know that our arenas are used more than what’s listed in that graph,” said Cindy Secord, who sits on the Ice Allocation Committee. “That graph should go in the other direction.”. \n",
      " Secord told CTV News that two minor ice hockey leagues have spent $450,000 buying 1,000 hours of ice time in neighbouring communities like Richmond or Delta. \n",
      " “We could not supply one sheet of ice for them for any games all year,” she said. “We’re turning away potential users because there’s nothing there.”. \n",
      " Surrey staff explained the numbers this way: “There are many possible reasons why the 2018 numbers appear low which may include waitlist accommodation, adding teaching stations, waitlist management. The numbers are being run from our stored data is not being modified.”. \n",
      " Several of the clubs are now writing Surrey City Council as it proceeds to its final approval of its budget on Monday night, asking the city to correct its figures. \n",
      " Viewing this article on our beta mobile site? Click here for a compatible version with embedded elements.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open (\"5c1452701e67d78e276ee126.txt\", \"r\", encoding='utf-8') as f: #open all files in the folder and read them\n",
    "        text = f.read()\n",
    "\n",
    "print(text)\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define the pattern to match phrases like 'said that', 'said', 'say that', or 'says that' followed by reported speech\n",
    "pattern_reported_speech = [{'LOWER':{'IN':['said', 'say']}},{'LOWER': 'that'},{'IS_ALPHA': True, 'OP': '*'},{'IS_PUNCT': True, \"OP\": \"*\"},{'IS_ALPHA': True, 'OP': '*'},{'ORTH':'.'}] \n",
    "\n",
    "matcher.add(\"REPORTED_SPEECH\", [pattern_reported_speech])\n",
    "doc = nlp(text)\n",
    "matches_reported_speech = matcher(doc)\n",
    "print(len(matches_reported_speech))\n",
    "for match in matches_reported_speech[:1]:\n",
    "    print(match, doc[match[1]:match[2]])\n",
    "    print(\"\\n\") #blank space between outputs\n",
    "\n",
    "\n",
    "#for match_id, start, end in matches_reported_speech:\n",
    "    #reported_speech = doc[end:].text  # Get the text starting from the end of the pattern match\n",
    "    #print(\"Reported speech found:\", reported_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae4f3c5-4091-426a-92bb-6ce53bcddd3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2022.05-py39",
   "language": "python",
   "name": "conda-env-anaconda-2022.05-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
