{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be347187",
   "metadata": {},
   "source": [
    "# Ling 450/807 SFU - Assignment 1\n",
    "\n",
    "This assignment walks you through two different ways of extracting simple quotes from text and then directs you to a third, already implemented way. Your task is to enhance the simple methods or develop your own. For further instructions, check the assignment file on Canvas. You'll need this notebook and the sample files in the A1_data directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cd57cf",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "\n",
    "We import everything we will need here at the beginning and load the spaCy language model. Note that we are using the small English model. One thing you could try is to download and load [other models for English](https://spacy.io/models/en) and compare the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65927718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d845f6",
   "metadata": {},
   "source": [
    "## Approach 1: Using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f2bcdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this loads and processes only one file at a time. You need to do 5 and comment on the results\n",
    "# to load the 5 texts, you can just change the name of the file below or figure out a way \n",
    "# to pass a list of files to the read command. It's up to you\n",
    "\n",
    "with open (\"A1_data/5c1dbe1d1e67d78e2797d611.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5755aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_sents(text):\n",
    "   # doc = nlp(text)\n",
    "    # sentences = list(doc.sents)\n",
    "    # return(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8218a1e",
   "metadata": {},
   "source": [
    "### Finding text within quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b26f91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quotes(text):\n",
    "    quotes = re.findall(r'[\"“](.*?)[”\"]', text)\n",
    "    return(quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10d48013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# found_sents = find_sents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e648a947",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Honestly, it feels like we're living our worst nightmare right now,\", 'The fact that we are being accused right now of an unethical adoption is crazy.', 'It does say that in the letter,', 'I have no idea where that information came from because both Clark and I were there in the office with all of the workers from the orphanage.', 'the Government of Canada has obligations under international conventions to ensure children are not abducted, bought or sold, or removed from their biological families without legal consent.', 'in some cases, extra steps in the citizenship or immigration process may be needed to make sure the adoption meets all requirements of international adoption.', \"If Canada doesn't grant him citizenship, what happens? They send him back to Nigeria to an orphanage? They take him from us even though he's legally our son?\", \"We're not giving up, but it feels really overwhelming to think about what this means and what they're trying to do to us right now,\", \"I can't believe that this is our life, that this is our story.\"]\n"
     ]
    }
   ],
   "source": [
    "# note: this just prints the text in quotes. If you want to save it locally\n",
    "# to analyze how the 3 approaches are different, you need to run a command to save\n",
    "# to a text file\n",
    "\n",
    "file = open (\"Assignment1_Output1.txt\", 'w')\n",
    "\n",
    "#for sent in found_sents:\n",
    "    #str_sent = str(sent)\n",
    "found_quotes = get_quotes(text)\n",
    "print(found_quotes)\n",
    "for quote in found_quotes:\n",
    "    file.write(str(quote) + \"\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "#    if len(found_quotes) > 0:\n",
    "#       print(found_quotes)\n",
    "#        file.write(str(found_quotes) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3153bbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fisherman']\n"
     ]
    }
   ],
   "source": [
    "# Attempts For Text1\n",
    "\n",
    "with open (\"A1_data/5c5d3f0a1e67d78e275e5788.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text2 = f.read() \n",
    "    \n",
    "def find_sents(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    return(sentences)\n",
    "\n",
    "def get_quotes(text):\n",
    "    quotes = re.findall(r' [“\"](.*?)[\"”]', text)\n",
    "    return(quotes)\n",
    "\n",
    "found_sents2 = find_sents(text2)\n",
    "\n",
    "file = open(\"Assignment1_Output2.txt\", 'w')\n",
    "for sent in found_sents2:\n",
    "    str_sent = str(sent)\n",
    "    found_quotes2 = get_quotes(str_sent)\n",
    "\n",
    "    if len(found_quotes2) > 0:\n",
    "        print(found_quotes2)\n",
    "        file.write(str(found_quotes2) + \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25227db3",
   "metadata": {},
   "source": [
    "## Approach 2: Using spaCy's Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b207ae63",
   "metadata": {},
   "source": [
    "This approach is based on notebooks by [William J.B. Mattingly](https://wjbmattingly.com/). His book, Introduction to Python for Humanists, is available online from the [SFU Library](https://sfu-primo.hosted.exlibrisgroup.com/permalink/f/usv8m3/01SFUL_ALMA51476999620003611). \n",
    "\n",
    "For more on spaCy's Matcher, see Advanced NLP with spaCy, [chapter 2](https://course.spacy.io/en/chapter2)). \n",
    "\n",
    "We have already loaded everything we need at the beginning of this notebook (imported Matcher, assigned it to a `matcher` object), so now we can use it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f0879",
   "metadata": {},
   "source": [
    "## Finding quotes and speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "033dc122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a text file. Remember, you have to do 5\n",
    "with open (\"A1_data/5c1dbe1d1e67d78e2797d611.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c4213f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert it to a spacy doc\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ee2f7c",
   "metadata": {},
   "source": [
    "### Finding proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd311bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(3232560085755078826, 111, 114) CTV News Friday\n"
     ]
    }
   ],
   "source": [
    "# This is optional. It just tells you who are the people mentioned. You can use it later if you want to find out the speakers of the quotes\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "#for token in doc:\n",
    "    #if token.pos_== 'PROPN'\n",
    "    \n",
    "pattern_n = [{\"POS\": \"PROPN\"},{\"POS\": \"PROPN\"},{\"POS\": \"PROPN\"}]\n",
    "\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern_n], greedy=\"LONGEST\")\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])\n",
    "    \n",
    "## You can try to extract full names by adding multi-word nouns, http://spacy.pythonhumanities.com/02_02_matcher.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f92f2",
   "metadata": {},
   "source": [
    "### Finding quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c4070a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ORTH': '\"'}, {'IS_ALPHA': True, 'OP': '+'}, {'ORTH': '\"'}]\n"
     ]
    },
    {
     "ename": "MatchPatternError",
     "evalue": "Invalid token patterns for matcher rule 'QUOTES'\n\nPattern 0:\n\n\nPattern 1:\n- [pattern -> 2 ->  ] extra fields not permitted\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMatchPatternError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(pattern_q)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#pattern_q = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}]\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#pattern_q1 = [{'ORTH': '“'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '”'}]\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m matcher\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQUOTES\u001b[39m\u001b[38;5;124m\"\u001b[39m, [pattern_q, pattern_q1], greedy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLONGEST\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(text)\n\u001b[1;32m     16\u001b[0m matches_q \u001b[38;5;241m=\u001b[39m matcher(doc)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/spacy/matcher/matcher.pyx:129\u001b[0m, in \u001b[0;36mspacy.matcher.matcher.Matcher.add\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mMatchPatternError\u001b[0m: Invalid token patterns for matcher rule 'QUOTES'\n\nPattern 0:\n\n\nPattern 1:\n- [pattern -> 2 ->  ] extra fields not permitted\n"
     ]
    }
   ],
   "source": [
    "# a simple pattern to extract things in single quotes\n",
    "# as with Approach 1, the for loop prints the results to the screen\n",
    "# you can try and save it to a file if you want to compare with Approach 1 and 3\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern_q = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"},{'ORTH': '\"'}]\n",
    "pattern_q1 = [{'ORTH': '\"'}, {'IS_PUNCT': True, \"OP\": \"+\"},{' ': True, \"OP\": \"+\"},{'ORTH': '\"'}]\n",
    "\n",
    "print(pattern_q)\n",
    "\n",
    "#pattern_q = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}]\n",
    "#pattern_q1 = [{'ORTH': '“'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '”'}]\n",
    "matcher.add(\"QUOTES\", [pattern_q, pattern_q1], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches_q = matcher(doc)\n",
    "matches_q.sort(key = lambda x: x[1])\n",
    "print (len(matches_q))\n",
    "for match in matches_q[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296dbcc1",
   "metadata": {},
   "source": [
    "## Approach 3: Implemented version\n",
    "This approach was implemented by colleagues at the [Australian Text Analytics Platform](https://www.atap.edu.au/) (ATAP). The approach is based on the [Gender Gap Tracker](https://github.com/sfu-discourse-lab/GenderGapTracker) done in the Discourse Processing Lab here at SFU. \n",
    "\n",
    "The first link below leads you to a binder where you can load your own files and download the output. If you prefer to do everything in your own notebook, you can download/clone the project from GitHub. \n",
    "\n",
    "* [Binder link](https://github.com/Australian-Text-Analytics-Platform/quotation-tool/blob/workshop_01_20220908/README.md)\n",
    "\n",
    "    * Click on the \"binder launch\" button.\n",
    "    * At the CILogin, under \"Select an Identity Provider\", go to the drop-down menu (usually default as ORCID) and select \"Simon Fraser University\".\n",
    "    * This launches [Binder](https://mybinder.readthedocs.io/en/latest/), a service that allows you to run a notebook online on Jupyter Lab (similar to Google Colab). \n",
    "    * Run all the code cells in that notebook, uploading files from the A1_data directory. \n",
    "    * At the end, you can save the output as an Excel file. \n",
    "\n",
    "* [Regular GitHub project](https://github.com/Australian-Text-Analytics-Platform/quotation-tool)\n",
    "\n",
    "    * Run the notebook \"quote_extractor_notebook.ipynb\"\n",
    "\n",
    "Within the ATAP binder, upload 5 files from A1_data (the same you did for approaches 1 and 2), process them and download the results to your own computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8674803d",
   "metadata": {},
   "source": [
    "## Your turn\n",
    "\n",
    "Check instructions on Canvas for what to do and what to submit. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ef8ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
