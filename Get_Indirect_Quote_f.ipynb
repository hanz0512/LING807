{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e5d6ab4",
   "metadata": {},
   "source": [
    "**In the approach, we would like to use the Matcher function as a starter.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebbbd60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy-wordnet in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (0.1.0)\n",
      "Requirement already satisfied: nltk<3.6,>=3.3 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy-wordnet) (3.5)\n",
      "Requirement already satisfied: spacy>=2 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy-wordnet) (3.7.2)\n",
      "Requirement already satisfied: click in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from nltk<3.6,>=3.3->spacy-wordnet) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from nltk<3.6,>=3.3->spacy-wordnet) (1.2.0)\n",
      "Requirement already satisfied: regex in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from nltk<3.6,>=3.3->spacy-wordnet) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from nltk<3.6,>=3.3->spacy-wordnet) (4.65.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (5.2.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/hanzhang/.local/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (1.26.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2->spacy-wordnet) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2->spacy-wordnet) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2->spacy-wordnet) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2->spacy-wordnet) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2->spacy-wordnet) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy>=2->spacy-wordnet) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy>=2->spacy-wordnet) (0.1.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy>=2->spacy-wordnet) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from jinja2->spacy>=2->spacy-wordnet) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "# one of the tools that suggested by Spacy is to get wordnet, so that we can retrieve synonyms of \"say\"-type verb\n",
    "\n",
    "# suggest by https://pypi.org/project/spacy-wordnet/\n",
    "\n",
    "!pip install spacy-wordnet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61883557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/hanzhang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    ">>> nltk.download('wordnet')\n",
    "\n",
    "import spacy\n",
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78d689ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b96c176",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <spacy_wordnet.wordnet_annotator.WordnetAnnotator object at 0x17704bc50> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m nlp\u001b[38;5;241m.\u001b[39madd_pipe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspacy_wordnet\u001b[39m\u001b[38;5;124m\"\u001b[39m, after\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtagger\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# spacy 2.X\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m nlp\u001b[38;5;241m.\u001b[39madd_pipe(WordnetAnnotator(nlp, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspacy_wordnet\u001b[39m\u001b[38;5;124m\"\u001b[39m), after\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtagger\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m wordnet_annotator \u001b[38;5;241m=\u001b[39m WordnetAnnotator()\n\u001b[1;32m     10\u001b[0m nlp\u001b[38;5;241m.\u001b[39madd_pipe(wordnet_annotator, after\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtagger\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/spacy/language.py:807\u001b[0m, in \u001b[0;36mLanguage.add_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    805\u001b[0m     bad_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrepr\u001b[39m(factory_name)\n\u001b[1;32m    806\u001b[0m     err \u001b[38;5;241m=\u001b[39m Errors\u001b[38;5;241m.\u001b[39mE966\u001b[38;5;241m.\u001b[39mformat(component\u001b[38;5;241m=\u001b[39mbad_val, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err)\n\u001b[1;32m    808\u001b[0m name \u001b[38;5;241m=\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m factory_name\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponent_names:\n",
      "\u001b[0;31mValueError\u001b[0m: [E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <spacy_wordnet.wordnet_annotator.WordnetAnnotator object at 0x17704bc50> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# spacy 3.X\n",
    "\n",
    "nlp.add_pipe(\"spacy_wordnet\", after='tagger') \n",
    "\n",
    "# spacy 2.X\n",
    "\n",
    "nlp.add_pipe(WordnetAnnotator(nlp, name=\"spacy_wordnet\"), after='tagger')\n",
    "\n",
    "wordnet_annotator = WordnetAnnotator()\n",
    "nlp.add_pipe(wordnet_annotator, after='tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16a48162",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "token = nlp('say')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73174a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('say.n.01.say'),\n",
       " Lemma('state.v.01.state'),\n",
       " Lemma('state.v.01.say'),\n",
       " Lemma('state.v.01.tell'),\n",
       " Lemma('allege.v.01.allege'),\n",
       " Lemma('allege.v.01.aver'),\n",
       " Lemma('allege.v.01.say'),\n",
       " Lemma('suppose.v.01.suppose'),\n",
       " Lemma('suppose.v.01.say'),\n",
       " Lemma('read.v.02.read'),\n",
       " Lemma('read.v.02.say'),\n",
       " Lemma('order.v.01.order'),\n",
       " Lemma('order.v.01.tell'),\n",
       " Lemma('order.v.01.enjoin'),\n",
       " Lemma('order.v.01.say'),\n",
       " Lemma('pronounce.v.01.pronounce'),\n",
       " Lemma('pronounce.v.01.articulate'),\n",
       " Lemma('pronounce.v.01.enounce'),\n",
       " Lemma('pronounce.v.01.sound_out'),\n",
       " Lemma('pronounce.v.01.enunciate'),\n",
       " Lemma('pronounce.v.01.say'),\n",
       " Lemma('say.v.07.say'),\n",
       " Lemma('say.v.08.say'),\n",
       " Lemma('say.v.09.say'),\n",
       " Lemma('say.v.10.say'),\n",
       " Lemma('say.v.11.say')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "token._.wordnet.synsets()\n",
    "token._.wordnet.lemmas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "713549a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['literature',\n",
       " 'book_keeping',\n",
       " 'pedagogy',\n",
       " 'factotum',\n",
       " 'psychiatry',\n",
       " 'person',\n",
       " 'sociology',\n",
       " 'philosophy',\n",
       " 'publishing',\n",
       " 'psychological_features',\n",
       " 'play',\n",
       " 'linguistics',\n",
       " 'law',\n",
       " 'commerce',\n",
       " 'theatre',\n",
       " 'theology',\n",
       " 'philology',\n",
       " 'psychology',\n",
       " 'enterprise',\n",
       " 'mathematics',\n",
       " 'religion',\n",
       " 'pure_science',\n",
       " 'economy',\n",
       " 'art',\n",
       " 'tax',\n",
       " 'quality',\n",
       " 'telecommunication',\n",
       " 'grammar',\n",
       " 'roman_catholic',\n",
       " 'literature',\n",
       " 'pedagogy',\n",
       " 'astronomy',\n",
       " 'pharmacy',\n",
       " 'heraldry',\n",
       " 'politics',\n",
       " 'philosophy',\n",
       " 'psychological_features',\n",
       " 'mythology',\n",
       " 'school',\n",
       " 'psychoanalysis',\n",
       " 'number',\n",
       " 'post',\n",
       " 'law',\n",
       " 'ethnology',\n",
       " 'theology',\n",
       " 'psychology',\n",
       " 'religion',\n",
       " 'archaeology',\n",
       " 'paleontology',\n",
       " 'paranormal',\n",
       " 'history',\n",
       " 'occultism',\n",
       " 'roman_catholic',\n",
       " 'hunting',\n",
       " 'artisanship',\n",
       " 'publishing',\n",
       " 'music',\n",
       " 'grammar',\n",
       " 'acoustics',\n",
       " 'linguistics',\n",
       " 'anatomy',\n",
       " 'mechanics',\n",
       " 'roman_catholic',\n",
       " 'roman_catholic',\n",
       " 'mechanics']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token._.wordnet.wordnet_domains()\n",
    "# to check which domain might fit better for finding synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3fc2336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She (tell|sound_out|say|pronounce|enounce|enunciate|state|articulate) that\n"
     ]
    }
   ],
   "source": [
    "say_related_domains = ['linguistics', 'grammar']\n",
    "enriched_sentence = []\n",
    "sentence = nlp('She says that')\n",
    "\n",
    "# For each token in the sentence\n",
    "for token in sentence:\n",
    "    # We get those synsets within the desired domains\n",
    "    synsets = token._.wordnet.wordnet_synsets_for_domain(say_related_domains)\n",
    "    if not synsets:\n",
    "        enriched_sentence.append(token.text)\n",
    "    else:\n",
    "        lemmas_for_synset = [lemma for s in synsets for lemma in s.lemma_names()]\n",
    "        # If we found a synset in the say_related_domains\n",
    "        # we get the variants and add them to the enriched sentence\n",
    "        enriched_sentence.append('({})'.format('|'.join(set(lemmas_for_synset))))\n",
    "\n",
    "# Let's see our enriched sentence\n",
    "print(' '.join(enriched_sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b358d5c8",
   "metadata": {},
   "source": [
    "**It seems that in spacy wordnet, the available synonyms are quite limited, thus we might want to create a Indir_speech_marker_list manully**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f60e6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[say, state, announce, sound_out, enunciate, tell, pronounce, articulate, claim, demonstrate, mention, according, state, suggest, find, note, cite, express, imply, describe, acknowledge, address, urge, refer]\n"
     ]
    }
   ],
   "source": [
    "# The list only consist of some of the quoting verbs as it is manually made\n",
    "Indir_speech_marker_list = [\"say\", \"state\", \"announce\", \"sound_out\" ,\"enunciate\", \"tell\", \"pronounce\", \"articulate\", \"claim\", \"demonstrate\", \"mention\", \"according\", \"state\", \"suggest\", \"find\", \"note\", \"cite\", \"express\", \"imply\", \"describe\", \"acknowledge\", \"address\", \"urge\", \"refer\"]\n",
    "\n",
    "# Process the list of words with Spacy\n",
    "IS_token_list = [token for word in Indir_speech_marker_list for token in nlp(word)]\n",
    "print(IS_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e886f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Matcher\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "with open (\"A1_data/5c1548a31e67d78e2771624f.txt\", \"r\", encoding='utf-8') as f: \n",
    "        text = f.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c48ceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PROBLEM!!!!!!!\n",
    "\n",
    "# pronoun pattern\n",
    "\n",
    "pattern = [{\"POS\": \"PROPN\", \"OP\": \"+\"}]\n",
    "matcher.add (\"PROPER_NOUNS\", [pattern], greedy = 'LONGEST')\n",
    "\n",
    "\n",
    "#Matcher start from the entity, pronoun identifier     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3eeb986",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 2) (1425827270.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[23], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    pattern_1 = [{\"ORTH\": \"”\"\"},\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 2)\n"
     ]
    }
   ],
   "source": [
    "# seperate direct and indirect\n",
    "Speech_lemma = [\"think\", \"say\", \"claim\"]\n",
    "pattern_1 = [{\"ORTH\": \"”\"\"},\n",
    "            {\"IS_ALPHA\":True, \"OP\": \"+\"},\n",
    "            {\"IS_PUNC\": True, \"OP\": \"*\"},\n",
    "            {\"ORTH\": \"'\"},\n",
    "            {\"POS\": \"VERB\", \"LEMMA\":{\"IN\": Speech_lemma}},\n",
    "            {\"POS\": \"PROPN\", \"OP\": \"+\"}, # Pronoun can be mutiple words\n",
    "            {\"ORTH\": \"”\"\"},\n",
    "            {\"IS_ALPHA\":True, \"OP\": \"+\"},\n",
    "            {\"IS_PUNC\": True, \"OP\": \"*\"},\n",
    "            {\"ORTH\": \"”\"\"}]\n",
    "print(pattern_1)\n",
    "\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort(key = lambda x: [1])\n",
    "matcher.add (\"Speech_Q\", [pattern_1], greedy = 'LONGEST')\n",
    "\n",
    "print(len(matches))\n",
    "for match in matches:\n",
    "    print(match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb54ea97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'IS_ENT': True}, {'POS': 'VERB', 'LEMMA': {'IN': ['say', 'state', 'announce', 'sound_out', 'enunciate', 'tell', 'pronounce', 'articulate', 'claim', 'demonstrate', 'mention', 'according', 'state', 'suggest', 'find', 'note', 'cite', 'express', 'imply', 'describe', 'acknowledge', 'address', 'urge', 'refer']}}]\n"
     ]
    }
   ],
   "source": [
    "# try to create a pattern which extract all the sentences that satisfy the following pattern\n",
    "#   A \"messager\"--marked as ENT, someone who re-utter the information\n",
    "# + A \"say-type\" word within a list of synonyms\n",
    "\n",
    "pattern = [{\"IS_ENT\": True}, \n",
    "           {\"POS\": \"VERB\", \"LEMMA\": {\"IN\": Indir_speech_marker_list}}\n",
    "          ]\n",
    "\n",
    "print (pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bab7607",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d934af03",
   "metadata": {},
   "outputs": [
    {
     "ename": "MatchPatternError",
     "evalue": "Invalid token patterns for matcher rule 'Indir'\n\nPattern 0:\n- [pattern -> 0 -> IS_ENT] extra fields not permitted\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMatchPatternError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m matcher\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIndir\u001b[39m\u001b[38;5;124m'\u001b[39m, [pattern])\n\u001b[1;32m      2\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(text)\n\u001b[1;32m      3\u001b[0m matches_IS \u001b[38;5;241m=\u001b[39m matcher(doc)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/spacy/matcher/matcher.pyx:129\u001b[0m, in \u001b[0;36mspacy.matcher.matcher.Matcher.add\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mMatchPatternError\u001b[0m: Invalid token patterns for matcher rule 'Indir'\n\nPattern 0:\n- [pattern -> 0 -> IS_ENT] extra fields not permitted\n"
     ]
    }
   ],
   "source": [
    "matcher.add('Indir', [pattern])\n",
    "doc = nlp(text)\n",
    "matches_IS = matcher(doc)\n",
    "print(len(Indir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703cf871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
