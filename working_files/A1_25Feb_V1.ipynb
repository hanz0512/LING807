{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be347187",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Ling 450/807 SFU - Assignment 1\n",
    "\n",
    "Virginia Uhi, Eunice Wong, & Han Zhang\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cd57cf",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Import packages\n",
    "\n",
    "We import everything we will need here at the beginning and load the spaCy language model. Note that we are using the small English model. One thing you could try is to download and load [other models for English](https://spacy.io/models/en) and compare the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65927718",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing Spacy \n",
    "import os\n",
    "import spacy\n",
    "import re\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d845f6",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Approach 1: Using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c2aea16-5b94-4392-a32b-d2ec31f72361",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m five_files \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA1_data/5c1548a31e67d78e2771624f.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA1_data/5c489df91e67d78e271d66c5.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA1_data/5c182ac21e67d78e277944ad.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA1_data/5c28972a795bd2fac69fa974.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA1_data/5c29beda1e67d78e27b74939.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Process five text files at the same time, and have all the quotes in the five files can be extracted and put in the same file\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mfive_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m output:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_quotes\u001b[39m(text):\n\u001b[1;32m     10\u001b[0m         quotes \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m“](.*?)[”\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m, text) \n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not list"
     ]
    }
   ],
   "source": [
    "# The starter file loads and processes only one file at a time. We are using the command to do 5 files at the same time\n",
    "\n",
    "# First we take five text files out \n",
    "\n",
    "five_files = [\"A1_data/5c1548a31e67d78e2771624f.txt\", \"A1_data/5c489df91e67d78e271d66c5.txt\", \"A1_data/5c182ac21e67d78e277944ad.txt\", \"A1_data/5c28972a795bd2fac69fa974.txt\", \"A1_data/5c29beda1e67d78e27b74939.txt\"]\n",
    "\n",
    "# Process five text files at the same time, and have all the quotes in the five files can be extracted and put in the same file\n",
    "with open (five_files, 'w', encoding = \"utf-8\") as output:\n",
    "    def get_quotes(text):\n",
    "        quotes = re.findall(r'[\"“](.*?)[”\"]', text) \n",
    "            # so that every time as soon as Spacy identify any quotation marks (either straight or curly), the quoted content with the quotation mark will be extracted\n",
    "        return(quotes)\n",
    "    \n",
    "    for file_path in five_files: \n",
    "        output.write(file_path + \"\\n\")\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "                # The starter code seperate the text into sentences at the begining; However, for the sake of better identifying the sentence, we decide to not seperate the sentences first\n",
    "\n",
    "            found_quotes = get_quotes(text) \n",
    "            print(found_quotes)\n",
    "            for quote in found_quotes:\n",
    "                output.write(str(quote) + \"\\n\")  # Using the \"\\n\" to make the quotes listed line by line\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25227db3",
   "metadata": {},
   "source": [
    "## Approach 2: Using spaCy's Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b207ae63",
   "metadata": {},
   "source": [
    "This approach is based on notebooks by [William J.B. Mattingly](https://wjbmattingly.com/). His book, Introduction to Python for Humanists, is available online from the [SFU Library](https://sfu-primo.hosted.exlibrisgroup.com/permalink/f/usv8m3/01SFUL_ALMA51476999620003611). \n",
    "\n",
    "For more on spaCy's Matcher, see Advanced NLP with spaCy, [chapter 2](https://course.spacy.io/en/chapter2)). \n",
    "\n",
    "We have already loaded everything we need at the beginning of this notebook (imported Matcher, assigned it to a `matcher` object), so now we can use it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f0879",
   "metadata": {},
   "source": [
    "## Finding quotes and speakers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ee2f7c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Finding proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd311bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "(3232560085755078826, 1, 2) CTV\n",
      "(3232560085755078826, 2, 3) Vancouver\n",
      "(3232560085755078826, 6, 7) Abbotsford\n",
      "(3232560085755078826, 8, 9) B.C.\n",
      "(3232560085755078826, 25, 26) Africa\n",
      "(3232560085755078826, 42, 43) Kim\n",
      "(3232560085755078826, 44, 45) Clark\n",
      "(3232560085755078826, 45, 46) Moran\n",
      "(3232560085755078826, 52, 53) Immigration\n",
      "(3232560085755078826, 54, 55) Refugees\n"
     ]
    }
   ],
   "source": [
    "# This is optional. It just tells you who are the people mentioned. You can use it later if you want to find out the speakers of the quotes\n",
    "\n",
    "# matcher = Matcher(nlp.vocab)\n",
    "# pattern_n = [{\"POS\": \"PROPN\"}]\n",
    "# matcher.add(\"PROPER_NOUNS\", [pattern_n], greedy=\"LONGEST\")\n",
    "# doc = nlp(text)\n",
    "# matches = matcher(doc)\n",
    "# print (len(matches))\n",
    "# for match in matches[:10]:\n",
    "    #print (match, doc[match[1]:match[2]])\n",
    "    \n",
    "## You can try to extract full names by adding multi-word nouns, http://spacy.pythonhumanities.com/02_02_matcher.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "debdbaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "(3232560085755078826, 0, 1) Australia\n",
      "(3232560085755078826, 9, 10) Canadians\n",
      "(3232560085755078826, 12, 13) China\n",
      "(3232560085755078826, 30, 31) Canada\n",
      "(3232560085755078826, 39, 44) Foreign Affairs Minister Marise Payne\n",
      "(3232560085755078826, 45, 46) Sunday\n",
      "(3232560085755078826, 55, 56) Australia\n",
      "(3232560085755078826, 71, 72) France\n",
      "(3232560085755078826, 73, 76) New York Times\n",
      "(3232560085755078826, 83, 84) China\n"
     ]
    }
   ],
   "source": [
    "# We escape the he code above and try another method\n",
    "# the starter code for extracting the propoer nouns encounter problem when the propoer noun contains mutiple words\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_n = [{\"POS\": \"PROPN\", \"OP\": \"+\" }] # with OP it will look for propoer noun one or more times\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern_n], greedy=\"LONGEST\")    \n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort (key = lambda x: x[1])  # in case that the results will be organized in descending order based on the length (given the greedy function)\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f92f2",
   "metadata": {},
   "source": [
    "### Finding quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4e50a4a-1e2b-4eda-90b5-7a4f03203b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".ipynb_checkpoints\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: 'A1_grp3_txt/.ipynb_checkpoints'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA1_grp3_txt\u001b[39m\u001b[38;5;124m\"\u001b[39m): \n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(file)\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mA1_grp3_txt/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f: \u001b[38;5;66;03m#open all files in the folder and read them\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      7\u001b[0m     matcher \u001b[38;5;241m=\u001b[39m Matcher(nlp\u001b[38;5;241m.\u001b[39mvocab)\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: 'A1_grp3_txt/.ipynb_checkpoints'"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(\"A1_grp3_txt\"): \n",
    "    print(file)\n",
    "\n",
    "    with open (\"A1_grp3_txt/\"+file, \"r\", encoding='utf-8') as f: #open all files in the folder and read them\n",
    "        text = f.read()\n",
    "    \n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    \n",
    "    pattern_q16 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #7\n",
    "    pattern_q17 = [{'ORTH': '“'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '”'}] #4\n",
    "    # both curly and straight quotes\n",
    "    \n",
    "    matcher.add(\"QUOTES\", [pattern_q16, pattern_q17])\n",
    "    doc = nlp(text)\n",
    "    matches_q = matcher(doc)\n",
    "    print(len(matches_q))\n",
    "    for match in matches_q[:10]:\n",
    "        print(match, doc[match[1]:match[2]])\n",
    "    print(\"\\n\") #blank space between outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08924137-c0da-4103-a581-aa7e075f5a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/a1a01287-00fd-4f66-9d41-05fe0b880957/LING807 Compuling\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3c4070a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "(16432004385153140588, 108, 116) \" Kim told CTV News Friday. \"\n",
      "(16432004385153140588, 115, 133) \"The fact that we are being accused right now of an unethical adoption is crazy.\"\n",
      "(16432004385153140588, 108, 133) \" Kim told CTV News Friday. \"The fact that we are being accused right now of an unethical adoption is crazy.\"\n",
      "(16432004385153140588, 164, 174) \"It does say that in the letter,\"\n",
      "(16432004385153140588, 173, 180) \" Kim confirmed, adding that \"\n",
      "(16432004385153140588, 179, 209) \"I have no idea where that information came from because both Clark and I were there in the office with all of the workers from the orphanage.\"\n",
      "(16432004385153140588, 280, 309) \"in some cases, extra steps in the citizenship or immigration process may be needed to make sure the adoption meets all requirements of international adoption.\"\n"
     ]
    }
   ],
   "source": [
    "# combination of different OP. \n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_q = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"!\"}, {'IS_PUNCT': True, \"OP\": \"!\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q1 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"!\"}, {'IS_PUNCT': True, \"OP\": \"?\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q2 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"!\"}, {'IS_PUNCT': True, \"OP\": \"+\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q3 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"!\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #0\n",
    "\n",
    "pattern_q4 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"?\"}, {'IS_PUNCT': True, \"OP\": \"!\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q5 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"?\"}, {'IS_PUNCT': True, \"OP\": \"?\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q6 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"?\"}, {'IS_PUNCT': True, \"OP\": \"+\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q7 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"?\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #0\n",
    "\n",
    "pattern_q8 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"!\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q9 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"?\"}, {'ORTH': '\"'}] #3\n",
    "pattern_q10 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"+\"}, {'ORTH': '\"'}] #3\n",
    "pattern_q11 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #3\n",
    "\n",
    "pattern_q12 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"!\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q13 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"?\"}, {'ORTH': '\"'}] #3\n",
    "pattern_q14 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"+\"}, {'ORTH': '\"'}] #3\n",
    "pattern_q15 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #3\n",
    "\n",
    "pattern_q16 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #7\n",
    "pattern_q17 = [{'ORTH': '“'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '”'}] #4\n",
    "\n",
    "#pattern_q4 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'ORTH': '\"'}]  #control\n",
    "#pattern_q5 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'ORTH': '.,!?': True, \"OP\": \"+\"}, {'ORTH': '\"'}] \n",
    "#pattern_q6 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'ORTH': '\"'}]\n",
    "#pattern_q7 = [{'ORTH': '\"'}, {'IS_ALPHA': True, 'OP': '?'}, {'ORTH': '\"'}]\n",
    "\n",
    "matcher.add(\"QUOTES\", [pattern_q16, pattern_q17])\n",
    "doc = nlp(text)\n",
    "matches_q = matcher(doc)\n",
    "# matches_q.sort(key = lambda x: x[1])\n",
    "print (len(matches_q))\n",
    "for match in matches_q[:10]:\n",
    "    print (match, doc[match[1]:match[2]])\n",
    "\n",
    "# MAYBE STRAIGHT/CURLY QUOTATION MARKS MATTER, SINCE IT IS RECOGNISING THE MIDDLE PART OF THE QUOTES AS WELL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296dbcc1",
   "metadata": {},
   "source": [
    "## Approach 3: Implemented version\n",
    "This approach was implemented by colleagues at the [Australian Text Analytics Platform](https://www.atap.edu.au/) (ATAP). The approach is based on the [Gender Gap Tracker](https://github.com/sfu-discourse-lab/GenderGapTracker) done in the Discourse Processing Lab here at SFU. \n",
    "\n",
    "The first link below leads you to a binder where you can load your own files and download the output. If you prefer to do everything in your own notebook, you can download/clone the project from GitHub. \n",
    "\n",
    "* [Binder link](https://github.com/Australian-Text-Analytics-Platform/quotation-tool/blob/workshop_01_20220908/README.md)\n",
    "\n",
    "    * Click on the \"binder launch\" button.\n",
    "    * At the CILogin, under \"Select an Identity Provider\", go to the drop-down menu (usually default as ORCID) and select \"Simon Fraser University\".\n",
    "    * This launches [Binder](https://mybinder.readthedocs.io/en/latest/), a service that allows you to run a notebook online on Jupyter Lab (similar to Google Colab). \n",
    "    * Run all the code cells in that notebook, uploading files from the A1_data directory. \n",
    "    * At the end, you can save the output as an Excel file. \n",
    "\n",
    "* [Regular GitHub project](https://github.com/Australian-Text-Analytics-Platform/quotation-tool)\n",
    "\n",
    "    * Run the notebook \"quote_extractor_notebook.ipynb\"\n",
    "\n",
    "Within the ATAP binder, upload 5 files from A1_data (the same you did for approaches 1 and 2), process them and download the results to your own computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8674803d",
   "metadata": {},
   "source": [
    "## Your turn\n",
    "\n",
    "Check instructions on Canvas for what to do and what to submit. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ef8ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2022.05-py39",
   "language": "python",
   "name": "conda-env-anaconda-2022.05-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
