{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be347187",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Ling 450/807 SFU - Assignment 1\n",
    "\n",
    "Virginia Uhi, Eunice Wong, & Han Zhang\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cd57cf",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Import packages\n",
    "\n",
    "We import everything we will need here at the beginning and load the spaCy language model. Note that we are using the small English model. One thing you could try is to download and load [other models for English](https://spacy.io/models/en) and compare the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65927718",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing Spacy \n",
    "import os\n",
    "import spacy\n",
    "import re\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d845f6",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Approach 1: Using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac4706-1e85-485b-970f-fc42ba202218",
   "metadata": {},
   "outputs": [],
   "source": [
    "five_files = [\"A1_data/5c1548a31e67d78e2771624f.txt\", \"A1_data/5c489df91e67d78e271d66c5.txt\", \"A1_data/5c182ac21e67d78e277944ad.txt\", \"A1_data/5c28972a795bd2fac69fa974.txt\", \"A1_data/5c29beda1e67d78e27b74939.txt\"]\n",
    "folder_path = [\"LING807 Compuling/A1_grp3_txt\"]\n",
    "\n",
    "for file in os.listdir(\"A1_grp3_txt\"): \n",
    "    print(file)\n",
    "\n",
    "#    with open (folder_path, 'w',encoding=\"utf-8\") as output:\n",
    "\n",
    "# Process five text files at the same time, and have all the quotes in the five files can be extracted and put in the same file\n",
    "#with open (five_files, 'w', encoding = \"utf-8\") as output:\n",
    "    def get_quotes(text):\n",
    "            quotes = re.findall(r'[\"“](.*?)[”\"]', text) \n",
    "            # so that every time as soon as Spacy identify any quotation marks (either straight or curly), the quoted content with the quotation mark will be extracted\n",
    "            return(quotes)\n",
    "    \n",
    "#    for file_path in five_files: \n",
    " #       output.write(file_path + \"\\n\")\n",
    "  #      with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "   #         text = file.read()\n",
    "                # The starter code seperate the text into sentences at the begining; However, for the sake of better identifying the sentence, we decide to not seperate the sentences first\n",
    "\n",
    "            found_quotes = get_quotes(text) \n",
    "            print(found_quotes)\n",
    "            for quote in found_quotes:\n",
    "                output.write(str(quote) + \"\\n\")  # Using the \"\\n\" to make the quotes listed line by line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2aea16-5b94-4392-a32b-d2ec31f72361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The starter file loads and processes only one file at a time. We are using the command to do 5 files at the same time\n",
    "\n",
    "# First we take five text files out \n",
    "\n",
    "five_files = [\"A1_data/5c1548a31e67d78e2771624f.txt\", \"A1_data/5c489df91e67d78e271d66c5.txt\", \"A1_data/5c182ac21e67d78e277944ad.txt\", \"A1_data/5c28972a795bd2fac69fa974.txt\", \"A1_data/5c29beda1e67d78e27b74939.txt\"]\n",
    "\n",
    "# Process five text files at the same time, and have all the quotes in the five files can be extracted and put in the same file\n",
    "with open (five_files, 'w', encoding = \"utf-8\") as output:\n",
    "    def get_quotes(text):\n",
    "        quotes = re.findall(r'[\"“](.*?)[”\"]', text) \n",
    "            # so that every time as soon as Spacy identify any quotation marks (either straight or curly), the quoted content with the quotation mark will be extracted\n",
    "        return(quotes)\n",
    "    \n",
    "    for file_path in five_files: \n",
    "        output.write(file_path + \"\\n\")\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "                # The starter code seperate the text into sentences at the begining; However, for the sake of better identifying the sentence, we decide to not seperate the sentences first\n",
    "\n",
    "            found_quotes = get_quotes(text) \n",
    "            print(found_quotes)\n",
    "            for quote in found_quotes:\n",
    "                output.write(str(quote) + \"\\n\")  # Using the \"\\n\" to make the quotes listed line by line\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25227db3",
   "metadata": {},
   "source": [
    "## Approach 2: Using spaCy's Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b207ae63",
   "metadata": {},
   "source": [
    "This approach is based on notebooks by [William J.B. Mattingly](https://wjbmattingly.com/). His book, Introduction to Python for Humanists, is available online from the [SFU Library](https://sfu-primo.hosted.exlibrisgroup.com/permalink/f/usv8m3/01SFUL_ALMA51476999620003611). \n",
    "\n",
    "For more on spaCy's Matcher, see Advanced NLP with spaCy, [chapter 2](https://course.spacy.io/en/chapter2)). \n",
    "\n",
    "We have already loaded everything we need at the beginning of this notebook (imported Matcher, assigned it to a `matcher` object), so now we can use it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f0879",
   "metadata": {},
   "source": [
    "## Finding quotes and speakers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ee2f7c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Finding proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd311bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is optional. It just tells you who are the people mentioned. You can use it later if you want to find out the speakers of the quotes\n",
    "\n",
    "# matcher = Matcher(nlp.vocab)\n",
    "# pattern_n = [{\"POS\": \"PROPN\"}]\n",
    "# matcher.add(\"PROPER_NOUNS\", [pattern_n], greedy=\"LONGEST\")\n",
    "# doc = nlp(text)\n",
    "# matches = matcher(doc)\n",
    "# print (len(matches))\n",
    "# for match in matches[:10]:\n",
    "    #print (match, doc[match[1]:match[2]])\n",
    "    \n",
    "## You can try to extract full names by adding multi-word nouns, http://spacy.pythonhumanities.com/02_02_matcher.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debdbaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We escape the he code above and try another method\n",
    "# the starter code for extracting the propoer nouns encounter problem when the propoer noun contains mutiple words\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_n = [{\"POS\": \"PROPN\", \"OP\": \"+\" }] # with OP it will look for propoer noun one or more times\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern_n], greedy=\"LONGEST\")    \n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort (key = lambda x: x[1])  # in case that the results will be organized in descending order based on the length (given the greedy function)\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f92f2",
   "metadata": {},
   "source": [
    "### Finding quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e50a4a-1e2b-4eda-90b5-7a4f03203b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(\"A1_grp3_txt\"): \n",
    "    print(file)\n",
    "\n",
    "    with open (\"A1_grp3_txt/\"+file, \"r\", encoding='utf-8') as f: #open all files in the folder and read them\n",
    "        text = f.read()\n",
    "    \n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    \n",
    "    pattern_q16 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #7\n",
    "    pattern_q17 = [{'ORTH': '“'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '”'}] #4\n",
    "    # both curly and straight quotes\n",
    "    \n",
    "    matcher.add(\"QUOTES\", [pattern_q16, pattern_q17])\n",
    "    doc = nlp(text)\n",
    "    matches_q = matcher(doc)\n",
    "    print(len(matches_q))\n",
    "    for match in matches_q[:10]:\n",
    "        print(match, doc[match[1]:match[2]])\n",
    "    print(\"\\n\") #blank space between outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08924137-c0da-4103-a581-aa7e075f5a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4070a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# combination of different OP. \n",
    "for file in os.listdir(\"A1_data\"): \n",
    "    print(file)\n",
    "\n",
    "    with open (\"A1_data/\"+file, \"r\", encoding='utf-8') as f: #open all files in the folder and read them\n",
    "        text = f.read()\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_q = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"!\"}, {'IS_PUNCT': True, \"OP\": \"!\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q1 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"!\"}, {'IS_PUNCT': True, \"OP\": \"?\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q2 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"!\"}, {'IS_PUNCT': True, \"OP\": \"+\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q3 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"!\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #0\n",
    "\n",
    "pattern_q4 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"?\"}, {'IS_PUNCT': True, \"OP\": \"!\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q5 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"?\"}, {'IS_PUNCT': True, \"OP\": \"?\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q6 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"?\"}, {'IS_PUNCT': True, \"OP\": \"+\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q7 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"?\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #0\n",
    "\n",
    "pattern_q8 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"!\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q9 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"?\"}, {'ORTH': '\"'}] #3\n",
    "pattern_q10 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"+\"}, {'ORTH': '\"'}] #3\n",
    "pattern_q11 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #3\n",
    "\n",
    "pattern_q12 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"!\"}, {'ORTH': '\"'}] #0\n",
    "pattern_q13 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"?\"}, {'ORTH': '\"'}] #3\n",
    "pattern_q14 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"+\"}, {'ORTH': '\"'}] #3\n",
    "pattern_q15 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #3\n",
    "\n",
    "pattern_q16 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}] #7\n",
    "pattern_q17 = [{'ORTH': '“'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '”'}] #4\n",
    "pattern_q18 = [{'LOWER':{'IN':['said', 'say','says']}},{'LOWER': 'that'}] #{'IS_ALPHA': True, 'OP': '*'},{'IS_PUNCT': True, \"OP\": \"*\"},{'IS_ALPHA': True, 'OP': '*'}]\n",
    "\n",
    "#pattern_q4 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'ORTH': '\"'}]  #control\n",
    "#pattern_q5 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'ORTH': '.,!?': True, \"OP\": \"+\"}, {'ORTH': '\"'}] \n",
    "#pattern_q6 = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"*\"}, {'ORTH': '\"'}]\n",
    "#pattern_q7 = [{'ORTH': '\"'}, {'IS_ALPHA': True, 'OP': '?'}, {'ORTH': '\"'}]\n",
    "\n",
    "matcher.add(\"QUOTES\", [pattern_q4])\n",
    "doc = nlp(text)\n",
    "matches_q = matcher(doc)\n",
    "# matches_q.sort(key = lambda x: x[1])\n",
    "print (len(matches_q))\n",
    "for match in matches_q[:10]:\n",
    "    print (match, doc[match[1]:match[2]])\n",
    "\n",
    "# MAYBE STRAIGHT/CURLY QUOTATION MARKS MATTER, SINCE IT IS RECOGNISING THE MIDDLE PART OF THE QUOTES AS WELL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296dbcc1",
   "metadata": {},
   "source": [
    "## Approach 3: Implemented version\n",
    "This approach was implemented by colleagues at the [Australian Text Analytics Platform](https://www.atap.edu.au/) (ATAP). The approach is based on the [Gender Gap Tracker](https://github.com/sfu-discourse-lab/GenderGapTracker) done in the Discourse Processing Lab here at SFU. \n",
    "\n",
    "The first link below leads you to a binder where you can load your own files and download the output. If you prefer to do everything in your own notebook, you can download/clone the project from GitHub. \n",
    "\n",
    "* [Binder link](https://github.com/Australian-Text-Analytics-Platform/quotation-tool/blob/workshop_01_20220908/README.md)\n",
    "\n",
    "    * Click on the \"binder launch\" button.\n",
    "    * At the CILogin, under \"Select an Identity Provider\", go to the drop-down menu (usually default as ORCID) and select \"Simon Fraser University\".\n",
    "    * This launches [Binder](https://mybinder.readthedocs.io/en/latest/), a service that allows you to run a notebook online on Jupyter Lab (similar to Google Colab). \n",
    "    * Run all the code cells in that notebook, uploading files from the A1_data directory. \n",
    "    * At the end, you can save the output as an Excel file. \n",
    "\n",
    "* [Regular GitHub project](https://github.com/Australian-Text-Analytics-Platform/quotation-tool)\n",
    "\n",
    "    * Run the notebook \"quote_extractor_notebook.ipynb\"\n",
    "\n",
    "Within the ATAP binder, upload 5 files from A1_data (the same you did for approaches 1 and 2), process them and download the results to your own computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8674803d",
   "metadata": {},
   "source": [
    "## New approach\n",
    "\n",
    "Check instructions on Canvas for what to do and what to submit. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ef8ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(\"A1_data\"): \n",
    "    print(file)\n",
    "\n",
    "    with open (\"A1_data\"+file, \"r\", encoding='utf-8') as f: #open all files in the folder and read them\n",
    "        text = f.read()\n",
    "    \n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    \n",
    "    pattern_1 = [{}] #7\n",
    "    pattern_q17 = [{'ORTH': '“'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '”'}] #4\n",
    "    # both curly and straight quotes\n",
    "    \n",
    "    matcher.add(\"QUOTES\", [pattern_q16, pattern_q17])\n",
    "    doc = nlp(text)\n",
    "    matches_q = matcher(doc)\n",
    "    print(len(matches_q))\n",
    "    for match in matches_q[:10]:\n",
    "        print(match, doc[match[1]:match[2]])\n",
    "    print(\"\\n\") #blank space between outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f584771f-4041-4994-b1a8-8fd3a371f898",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define the pattern to match reported speech or indirect quotes\n",
    "pattern_reported_speech = [\n",
    "    {'LOWER': {'IN': ['said', 'say']}},\n",
    "    {'LOWER': 'that'},\n",
    "    {'POS': {'IN': ['ADP', 'NOUN', 'PRON', 'PROPN']}, 'OP': '*'},  # Optionally match any preposition, noun, pronoun, or proper noun\n",
    "    {'IS_ALPHA': True, 'OP': '+'}  # Match one or more alphabetical characters (the reported speech)\n",
    "]\n",
    "\n",
    "matcher.add(\"REPORTED_SPEECH\", [pattern_reported_speech])\n",
    "\n",
    "# Example text containing reported speech\n",
    "text = \"He said that he would come tomorrow. She says that she likes ice cream. They said, 'We are going to the beach.'\"\n",
    "\n",
    "doc = nlp(text)\n",
    "matches_reported_speech = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches_reported_speech:\n",
    "    reported_speech = doc[start:end].text\n",
    "    print(\"Reported speech found:\", reported_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b8efd8-8d29-4cc5-957c-eda5f3d53fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open (\"5c1452701e67d78e276ee126.txt\", \"r\", encoding='utf-8') as f: #open all files in the folder and read them\n",
    "        text = f.read()\n",
    "\n",
    "print(text)\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define the pattern to match phrases like 'said that', 'said', 'say that', or 'says that' followed by reported speech\n",
    "pattern_reported_speech = [{'LOWER':{'IN':['said', 'say']}},{'LOWER': 'that'},{'IS_ALPHA': True, 'OP': '*'},{'IS_PUNCT': True, \"OP\": \"*\"},{'IS_ALPHA': True, 'OP': '*'},{'ORTH':'.'}] \n",
    "\n",
    "matcher.add(\"REPORTED_SPEECH\", [pattern_reported_speech])\n",
    "doc = nlp(text)\n",
    "matches_reported_speech = matcher(doc)\n",
    "print(len(matches_reported_speech))\n",
    "for match in matches_reported_speech[:1]:\n",
    "    print(match, doc[match[1]:match[2]])\n",
    "    print(\"\\n\") #blank space between outputs\n",
    "\n",
    "\n",
    "#for match_id, start, end in matches_reported_speech:\n",
    "    #reported_speech = doc[end:].text  # Get the text starting from the end of the pattern match\n",
    "    #print(\"Reported speech found:\", reported_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae4f3c5-4091-426a-92bb-6ce53bcddd3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
