{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e5d6ab4",
   "metadata": {},
   "source": [
    "**In the approach, we would like to use the Matcher function as a starter.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ebbbd60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy-wordnet in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (0.1.0)\n",
      "Requirement already satisfied: nltk<3.6,>=3.3 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy-wordnet) (3.5)\n",
      "Requirement already satisfied: spacy>=2 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy-wordnet) (3.7.2)\n",
      "Requirement already satisfied: click in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from nltk<3.6,>=3.3->spacy-wordnet) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from nltk<3.6,>=3.3->spacy-wordnet) (1.2.0)\n",
      "Requirement already satisfied: regex in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from nltk<3.6,>=3.3->spacy-wordnet) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from nltk<3.6,>=3.3->spacy-wordnet) (4.65.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (5.2.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/hanzhang/.local/lib/python3.11/site-packages (from spacy>=2->spacy-wordnet) (1.26.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2->spacy-wordnet) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2->spacy-wordnet) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2->spacy-wordnet) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2->spacy-wordnet) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2->spacy-wordnet) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy>=2->spacy-wordnet) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy>=2->spacy-wordnet) (0.1.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy>=2->spacy-wordnet) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/hanzhang/anaconda3/lib/python3.11/site-packages (from jinja2->spacy>=2->spacy-wordnet) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "# one of the tools that suggested by Spacy is to get wordnet, so that we can retrieve synonyms of \"say\"-type verb\n",
    "\n",
    "# suggest by https://pypi.org/project/spacy-wordnet/\n",
    "\n",
    "!pip install spacy-wordnet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "61883557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/hanzhang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    ">>> nltk.download('wordnet')\n",
    "\n",
    "import spacy\n",
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "78d689ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b96c176",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <spacy_wordnet.wordnet_annotator.WordnetAnnotator object at 0x2a3ee8a90> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m nlp\u001b[38;5;241m.\u001b[39madd_pipe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspacy_wordnet\u001b[39m\u001b[38;5;124m\"\u001b[39m, after\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtagger\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# spacy 2.X\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m nlp\u001b[38;5;241m.\u001b[39madd_pipe(WordnetAnnotator(nlp, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspacy_wordnet\u001b[39m\u001b[38;5;124m\"\u001b[39m), after\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtagger\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m wordnet_annotator \u001b[38;5;241m=\u001b[39m WordnetAnnotator()\n\u001b[1;32m     10\u001b[0m nlp\u001b[38;5;241m.\u001b[39madd_pipe(wordnet_annotator, after\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtagger\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/spacy/language.py:807\u001b[0m, in \u001b[0;36mLanguage.add_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    805\u001b[0m     bad_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrepr\u001b[39m(factory_name)\n\u001b[1;32m    806\u001b[0m     err \u001b[38;5;241m=\u001b[39m Errors\u001b[38;5;241m.\u001b[39mE966\u001b[38;5;241m.\u001b[39mformat(component\u001b[38;5;241m=\u001b[39mbad_val, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err)\n\u001b[1;32m    808\u001b[0m name \u001b[38;5;241m=\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m factory_name\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponent_names:\n",
      "\u001b[0;31mValueError\u001b[0m: [E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <spacy_wordnet.wordnet_annotator.WordnetAnnotator object at 0x2a3ee8a90> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# spacy 3.X\n",
    "\n",
    "nlp.add_pipe(\"spacy_wordnet\", after='tagger') \n",
    "\n",
    "# spacy 2.X\n",
    "\n",
    "nlp.add_pipe(WordnetAnnotator(nlp, name=\"spacy_wordnet\"), after='tagger')\n",
    "\n",
    "wordnet_annotator = WordnetAnnotator()\n",
    "nlp.add_pipe(wordnet_annotator, after='tagger')\n",
    "\n",
    "# the website suggested the two models and we implemented them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "16a48162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the variable, we speculate that if we put more \"quoting verbs\" in the dict, it is going to create a bigger list\n",
    "token = nlp('say')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "73174a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('say.n.01.say'),\n",
       " Lemma('state.v.01.state'),\n",
       " Lemma('state.v.01.say'),\n",
       " Lemma('state.v.01.tell'),\n",
       " Lemma('allege.v.01.allege'),\n",
       " Lemma('allege.v.01.aver'),\n",
       " Lemma('allege.v.01.say'),\n",
       " Lemma('suppose.v.01.suppose'),\n",
       " Lemma('suppose.v.01.say'),\n",
       " Lemma('read.v.02.read'),\n",
       " Lemma('read.v.02.say'),\n",
       " Lemma('order.v.01.order'),\n",
       " Lemma('order.v.01.tell'),\n",
       " Lemma('order.v.01.enjoin'),\n",
       " Lemma('order.v.01.say'),\n",
       " Lemma('pronounce.v.01.pronounce'),\n",
       " Lemma('pronounce.v.01.articulate'),\n",
       " Lemma('pronounce.v.01.enounce'),\n",
       " Lemma('pronounce.v.01.sound_out'),\n",
       " Lemma('pronounce.v.01.enunciate'),\n",
       " Lemma('pronounce.v.01.say'),\n",
       " Lemma('say.v.07.say'),\n",
       " Lemma('say.v.08.say'),\n",
       " Lemma('say.v.09.say'),\n",
       " Lemma('say.v.10.say'),\n",
       " Lemma('say.v.11.say')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The \"extract\" function: system sets and lemmas\n",
    "token._.wordnet.synsets()\n",
    "token._.wordnet.lemmas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "713549a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['literature',\n",
       " 'book_keeping',\n",
       " 'pedagogy',\n",
       " 'factotum',\n",
       " 'psychiatry',\n",
       " 'person',\n",
       " 'sociology',\n",
       " 'philosophy',\n",
       " 'publishing',\n",
       " 'psychological_features',\n",
       " 'play',\n",
       " 'linguistics',\n",
       " 'law',\n",
       " 'commerce',\n",
       " 'theatre',\n",
       " 'theology',\n",
       " 'philology',\n",
       " 'psychology',\n",
       " 'enterprise',\n",
       " 'mathematics',\n",
       " 'religion',\n",
       " 'pure_science',\n",
       " 'economy',\n",
       " 'art',\n",
       " 'tax',\n",
       " 'quality',\n",
       " 'telecommunication',\n",
       " 'grammar',\n",
       " 'roman_catholic',\n",
       " 'literature',\n",
       " 'pedagogy',\n",
       " 'astronomy',\n",
       " 'pharmacy',\n",
       " 'heraldry',\n",
       " 'politics',\n",
       " 'philosophy',\n",
       " 'psychological_features',\n",
       " 'mythology',\n",
       " 'school',\n",
       " 'psychoanalysis',\n",
       " 'number',\n",
       " 'post',\n",
       " 'law',\n",
       " 'ethnology',\n",
       " 'theology',\n",
       " 'psychology',\n",
       " 'religion',\n",
       " 'archaeology',\n",
       " 'paleontology',\n",
       " 'paranormal',\n",
       " 'history',\n",
       " 'occultism',\n",
       " 'roman_catholic',\n",
       " 'hunting',\n",
       " 'artisanship',\n",
       " 'publishing',\n",
       " 'music',\n",
       " 'grammar',\n",
       " 'acoustics',\n",
       " 'linguistics',\n",
       " 'anatomy',\n",
       " 'mechanics',\n",
       " 'roman_catholic',\n",
       " 'roman_catholic',\n",
       " 'mechanics']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token._.wordnet.wordnet_domains()\n",
    "# to check which domain might fit better for finding synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d3fc2336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She (tell|sound_out|say|pronounce|enounce|enunciate|state|articulate) that\n"
     ]
    }
   ],
   "source": [
    "# we selected the \"linguistics\" and \"grammar\" domains, we speculate that if we choose more domains, we will get more results\n",
    "say_related_domains = ['linguistics', 'grammar']\n",
    "enriched_sentence = []\n",
    "sentence = nlp('She says that')\n",
    "\n",
    "# For each token in the sentence\n",
    "for token in sentence:\n",
    "    # We get those synsets within the desired domains\n",
    "    synsets = token._.wordnet.wordnet_synsets_for_domain(say_related_domains)\n",
    "    if not synsets:\n",
    "        enriched_sentence.append(token.text)\n",
    "    else:\n",
    "        lemmas_for_synset = [lemma for s in synsets for lemma in s.lemma_names()]\n",
    "        # If we found a synset in the say_related_domains\n",
    "        # we get the variants and add them to the enriched sentence\n",
    "        enriched_sentence.append('({})'.format('|'.join(set(lemmas_for_synset))))\n",
    "\n",
    "# Let's see our enriched sentence\n",
    "print(' '.join(enriched_sentence))\n",
    "\n",
    "# the two domains only gave us these \"quoting verbs\" (there are more synonyms that are not listed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b358d5c8",
   "metadata": {},
   "source": [
    "**It seems that in spacy wordnet, the available synonyms are quite limited, thus we might want to create a Indir_speech_marker_list manully**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2f60e6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[say, state, announce, sound_out, enunciate, tell, pronounce, articulate, claim, demonstrate, mention, according, state, suggest, find, note, cite, express, imply, describe, acknowledge, address, urge, refer]\n"
     ]
    }
   ],
   "source": [
    "# The list only consist of some of the quoting verbs as it is manually made\n",
    "Indir_speech_marker_list = [\"say\", \"state\", \"announce\", \"sound_out\" ,\"enunciate\", \"tell\", \"pronounce\", \"articulate\", \"claim\", \"demonstrate\", \"mention\", \"according\", \"state\", \"suggest\", \"find\", \"note\", \"cite\", \"express\", \"imply\", \"describe\", \"acknowledge\", \"address\", \"urge\", \"refer\"]\n",
    "\n",
    "# Process the list of words with Spacy\n",
    "IS_token_list = [token for word in Indir_speech_marker_list for token in nlp(word)]\n",
    "print(IS_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6e886f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Matcher\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "#open one text file and try\n",
    "with open (\"A1_data/5c1548a31e67d78e2771624f.txt\", \"r\", encoding='utf-8') as f: \n",
    "        text = f.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4c48ceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we try to create a pattern which extract all the sentences that satisfy the following pattern\n",
    "#   A \"messager\"--marked as ENT, someone who reiterate the information\n",
    "# + A \"say-type\" word within a list of synonyms in the listed we made above\n",
    "\n",
    "\n",
    "pattern = [{\"POS\": \"PROPN\", \"OP\": \"*\"}, # looking for a messager\n",
    "          {\"POS\": \"VERB\", \"LEMMA\":{\"IN\": Indir_speech_marker_list}}, # find the lemma form of the verb that fits in the list\n",
    "          {\"IS_ALPHA\":True, \"OP\": \"*\"},\n",
    "          {\"ORTH\":{\"IN\": [\"\\.\",\"\\!\", \"\\?\"]}} # until we get to the sentence boundary\n",
    "           ]\n",
    "matcher.add (\"PROPER_NOUNS\", [pattern], greedy = 'LONGEST')\n",
    "\n",
    "\n",
    "#Matcher start from the entity, pronoun identifier     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b3eeb986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# seperate direct and indirect\n",
    "\n",
    "\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort(key = lambda x: [1])\n",
    "matcher.add (\"Indir\", [pattern], greedy = 'LONGEST')\n",
    "\n",
    "\n",
    "print(len(matches))\n",
    "for match_id, start, end in matches:\n",
    "    print(doc[start:end])\n",
    "\n",
    "#    print(len(matches))\n",
    "#    for match in matches:\n",
    "   # print(match, doc[match[1]:match[2]])\n",
    "    \n",
    "    # oh no this is not working gg :( oops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d934af03",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Indir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(text)\n\u001b[1;32m      3\u001b[0m matches_IS \u001b[38;5;241m=\u001b[39m matcher(doc)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(Indir))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Indir' is not defined"
     ]
    }
   ],
   "source": [
    "matcher.add(\"Indir\", [pattern])\n",
    "doc = nlp(text)\n",
    "matches_IS = matcher(doc)\n",
    "print(len(Indir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703cf871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
