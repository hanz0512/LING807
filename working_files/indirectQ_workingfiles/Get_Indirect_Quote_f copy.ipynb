{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e5d6ab4",
   "metadata": {},
   "source": [
    "**In the approach, we would like to use the Matcher function as a starter.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbbd60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one of the tools that suggested by Spacy is to get wordnet, so that we can retrieve synonyms of \"say\"-type verb\n",
    "\n",
    "# suggest by https://pypi.org/project/spacy-wordnet/\n",
    "\n",
    "!pip install spacy-wordnet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61883557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    ">>> nltk.download('wordnet')\n",
    "\n",
    "import spacy\n",
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d689ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b96c176",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# spacy 3.X\n",
    "\n",
    "nlp.add_pipe(\"spacy_wordnet\", after='tagger') \n",
    "\n",
    "# spacy 2.X\n",
    "\n",
    "nlp.add_pipe(WordnetAnnotator(nlp, name=\"spacy_wordnet\"), after='tagger')\n",
    "\n",
    "wordnet_annotator = WordnetAnnotator()\n",
    "nlp.add_pipe(wordnet_annotator, after='tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a48162",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "token = nlp('say')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73174a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "token._.wordnet.synsets()\n",
    "token._.wordnet.lemmas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713549a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "token._.wordnet.wordnet_domains()\n",
    "# to check which domain might fit better for finding synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fc2336",
   "metadata": {},
   "outputs": [],
   "source": [
    "say_related_domains = ['linguistics', 'grammar']\n",
    "enriched_sentence = []\n",
    "sentence = nlp('She says that')\n",
    "\n",
    "# For each token in the sentence\n",
    "for token in sentence:\n",
    "    # We get those synsets within the desired domains\n",
    "    synsets = token._.wordnet.wordnet_synsets_for_domain(say_related_domains)\n",
    "    if not synsets:\n",
    "        enriched_sentence.append(token.text)\n",
    "    else:\n",
    "        lemmas_for_synset = [lemma for s in synsets for lemma in s.lemma_names()]\n",
    "        # If we found a synset in the say_related_domains\n",
    "        # we get the variants and add them to the enriched sentence\n",
    "        enriched_sentence.append('({})'.format('|'.join(set(lemmas_for_synset))))\n",
    "\n",
    "# Let's see our enriched sentence\n",
    "print(' '.join(enriched_sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b358d5c8",
   "metadata": {},
   "source": [
    "**It seems that in spacy wordnet, the available synonyms are quite limited, thus we might want to create a Indir_speech_marker_list manully**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f60e6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list only consist of some of the quoting verbs as it is manually made\n",
    "Indir_speech_marker_list = [\"say\", \"state\", \"announce\", \"sound_out\" ,\"enunciate\", \"tell\", \"pronounce\", \"articulate\", \"claim\", \"demonstrate\", \"mention\", \"according\", \"state\", \"suggest\", \"find\", \"note\", \"cite\", \"express\", \"imply\", \"describe\", \"acknowledge\", \"address\", \"urge\", \"refer\"]\n",
    "#use this list when making a pattern\n",
    "\n",
    "\n",
    "# Process the list of words with Spacy\n",
    "IS_token_list = [token for word in Indir_speech_marker_list for token in nlp(word)]\n",
    "print(IS_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e886f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Matcher\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "with open (\"A1_data/5c1548a31e67d78e2771624f.txt\", \"r\", encoding='utf-8') as f: \n",
    "        text = f.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c48ceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PROBLEM!!!!!!!\n",
    "\n",
    "# pronoun pattern\n",
    "\n",
    "pattern = [{\"POS\": \"PROPN\", \"OP\": \"+\"}]\n",
    "matcher.add (\"PROPER_NOUNS\", [pattern], greedy = 'LONGEST')\n",
    "\n",
    "\n",
    "#Matcher start from the entity, pronoun identifier     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eeb986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate direct and indirect\n",
    "Speech_lemma = [\"think\", \"say\", \"claim\"]\n",
    "pattern_1 = [{\"ORTH\": \"”\"\"},\n",
    "            {\"IS_ALPHA\":True, \"OP\": \"+\"},\n",
    "            {\"IS_PUNC\": True, \"OP\": \"*\"},\n",
    "            {\"ORTH\": \"'\"},\n",
    "            {\"POS\": \"VERB\", \"LEMMA\":{\"IN\": Speech_lemma}},\n",
    "            {\"POS\": \"PROPN\", \"OP\": \"+\"}, # Pronoun can be mutiple words\n",
    "            {\"ORTH\": \"”\"\"},\n",
    "            {\"IS_ALPHA\":True, \"OP\": \"+\"},\n",
    "            {\"IS_PUNC\": True, \"OP\": \"*\"},\n",
    "            {\"ORTH\": \"”\"\"}]\n",
    "print(pattern_1)\n",
    "\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort(key = lambda x: [1])\n",
    "matcher.add (\"Speech_Q\", [pattern_1], greedy = 'LONGEST')\n",
    "\n",
    "print(len(matches))\n",
    "for match in matches:\n",
    "    print(match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb54ea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to create a pattern which extract all the sentences that satisfy the following pattern\n",
    "#   A \"messager\"--marked as ENT, someone who re-utter the information\n",
    "# + A \"say-type\" word within a list of synonyms\n",
    "\n",
    "pattern = [{\"IS_ENT\": True}, \n",
    "           {\"POS\": \"VERB\", \"LEMMA\": {\"IN\": Indir_speech_marker_list}}\n",
    "          ]\n",
    "\n",
    "print (pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bab7607",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d934af03",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add('Indir', [pattern])\n",
    "doc = nlp(text)\n",
    "matches_IS = matcher(doc)\n",
    "print(len(Indir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703cf871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
